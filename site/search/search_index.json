{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Serving Models","text":""},{"location":"#introduction","title":"Introduction","text":"<p>TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. TensorFlow Serving provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve other types of models and data.</p> <p>Detailed developer documentation on TensorFlow Serving is available:</p> <ul> <li>Architecture Overview</li> <li>Server API</li> <li>REST Client API</li> </ul>"},{"location":"api/api_rest/","title":"RESTful API","text":"<p>In addition to gRPC APIs TensorFlow ModelServer also supports RESTful APIs. This page describes these API endpoints and an end-to-end example on usage.</p> <p>The request and response is a JSON object. The composition of this object depends on the request type or verb. See the API specific sections below for details.</p> <p>In case of error, all APIs will return a JSON object in the response body with <code>error</code> as key and the error message as the value:</p> <pre><code>{\n  \"error\": &lt;error message string&gt;\n}\n</code></pre>"},{"location":"api/api_rest/#model-status-api","title":"Model status API","text":"<p>This API closely follows the <code>ModelService.GetModelStatus</code> gRPC API. It returns the status of a model in the ModelServer.</p>"},{"location":"api/api_rest/#url","title":"URL","text":"<pre><code>GET http://host:port/v1/models/${MODEL_NAME}[/versions/${VERSION}|/labels/${LABEL}]\n</code></pre> <p>Including <code>/versions/${VERSION}</code> or <code>/labels/${LABEL}</code> is optional. If omitted status for all versions is returned in the response.</p>"},{"location":"api/api_rest/#response-format","title":"Response format","text":"<p>If successful, returns a JSON representation of <code>GetModelStatusResponse</code> protobuf.</p>"},{"location":"api/api_rest/#model-metadata-api","title":"Model Metadata API","text":"<p>This API closely follows the <code>PredictionService.GetModelMetadata</code> gRPC API. It returns the metadata of a model in the ModelServer.</p>"},{"location":"api/api_rest/#url_1","title":"URL","text":"<pre><code>GET http://host:port/v1/models/${MODEL_NAME}[/versions/${VERSION}|/labels/${LABEL}]/metadata\n</code></pre> <p>Including <code>/versions/${VERSION}</code> or <code>/labels/${LABEL}</code> is optional. If omitted the model metadata for the latest version is returned in the response.</p>"},{"location":"api/api_rest/#response-format_1","title":"Response format","text":"<p>If successful, returns a JSON representation of <code>GetModelMetadataResponse</code> protobuf.</p>"},{"location":"api/api_rest/#classify-and-regress-api","title":"Classify and Regress API","text":"<p>This API closely follows the <code>Classify</code> and <code>Regress</code> methods of <code>PredictionService</code> gRPC API.</p>"},{"location":"api/api_rest/#url_2","title":"URL","text":"<pre><code>POST http://host:port/v1/models/${MODEL_NAME}[/versions/${VERSION}|/labels/${LABEL}]:(classify|regress)\n</code></pre> <p>Including <code>/versions/${VERSION}</code> or <code>/labels/${LABEL}</code> is optional. If omitted the latest version is used.</p>"},{"location":"api/api_rest/#request-format","title":"Request format","text":"<p>The request body for the <code>classify</code> and <code>regress</code> APIs must be a JSON object formatted as follows:</p> <pre><code>{\n  // Optional: serving signature to use.\n  // If unspecifed default serving signature is used.\n  \"signature_name\": &lt;string&gt;,\n\n  // Optional: Common context shared by all examples.\n  // Features that appear here MUST NOT appear in examples (below).\n  \"context\": {\n    \"&lt;feature_name3&gt;\": &lt;value&gt;|&lt;list&gt;\n    \"&lt;feature_name4&gt;\": &lt;value&gt;|&lt;list&gt;\n  },\n\n  // List of Example objects\n  \"examples\": [\n    {\n      // Example 1\n      \"&lt;feature_name1&gt;\": &lt;value&gt;|&lt;list&gt;,\n      \"&lt;feature_name2&gt;\": &lt;value&gt;|&lt;list&gt;,\n      ...\n    },\n    {\n      // Example 2\n      \"&lt;feature_name1&gt;\": &lt;value&gt;|&lt;list&gt;,\n      \"&lt;feature_name2&gt;\": &lt;value&gt;|&lt;list&gt;,\n      ...\n    }\n    ...\n  ]\n}\n</code></pre> <p><code>&lt;value&gt;</code> is a JSON number (whole or decimal), JSON string, or a JSON object that represents binary data (see the Encoding binary values section below for details). <code>&lt;list&gt;</code> is a list of such values. This format is similar to gRPC's <code>ClassificationRequest</code> and <code>RegressionRequest</code> protos. Both versions accept list of <code>Example</code> objects.</p>"},{"location":"api/api_rest/#response-format_2","title":"Response format","text":"<p>A <code>classify</code> request returns a JSON object in the response body, formatted as follows:</p> <pre><code>{\n  \"result\": [\n    // List of class label/score pairs for first Example (in request)\n    [ [&lt;label1&gt;, &lt;score1&gt;], [&lt;label2&gt;, &lt;score2&gt;], ... ],\n\n    // List of class label/score pairs for next Example (in request)\n    [ [&lt;label1&gt;, &lt;score1&gt;], [&lt;label2&gt;, &lt;score2&gt;], ... ],\n    ...\n  ]\n}\n</code></pre> <p><code>&lt;label&gt;</code> is a string (which can be an empty string <code>\"\"</code> if the model does not have a label associated with the score). <code>&lt;score&gt;</code> is a decimal (floating point) number.</p> <p>The <code>regress</code> request returns a JSON object in the response body, formatted as follows:</p> <pre><code>{\n  // One regression value for each example in the request in the same order.\n  \"result\": [ &lt;value1&gt;, &lt;value2&gt;, &lt;value3&gt;, ...]\n}\n</code></pre> <p><code>&lt;value&gt;</code> is a decimal number.</p> <p>Users of gRPC API will notice the similarity of this format with <code>ClassificationResponse</code> and <code>RegressionResponse</code> protos.</p>"},{"location":"api/api_rest/#predict-api","title":"Predict API","text":"<p>This API closely follows the <code>PredictionService.Predict</code> gRPC API.</p>"},{"location":"api/api_rest/#url_3","title":"URL","text":"<pre><code>POST http://host:port/v1/models/${MODEL_NAME}[/versions/${VERSION}|/labels/${LABEL}]:predict\n</code></pre> <p>Including <code>/versions/${VERSION}</code> or <code>/labels/${LABEL}</code> is optional. If omitted the latest version is used.</p>"},{"location":"api/api_rest/#request-format_1","title":"Request format","text":"<p>The request body for <code>predict</code> API must be JSON object formatted as follows:</p> <pre><code>{\n  // (Optional) Serving signature to use.\n  // If unspecifed default serving signature is used.\n  \"signature_name\": &lt;string&gt;,\n\n  // Input Tensors in row (\"instances\") or columnar (\"inputs\") format.\n  // A request can have either of them but NOT both.\n  \"instances\": &lt;value&gt;|&lt;(nested)list&gt;|&lt;list-of-objects&gt;\n  \"inputs\": &lt;value&gt;|&lt;(nested)list&gt;|&lt;object&gt;\n}\n</code></pre>"},{"location":"api/api_rest/#specifying-input-tensors-in-row-format","title":"Specifying input tensors in row format.","text":"<p>This format is similar to <code>PredictRequest</code> proto of gRPC API and the CMLE predict API. Use this format if all named input tensors have the same 0-th dimension. If they don't, use the columnar format described later below.</p> <p>In the row format, inputs are keyed to instances key in the JSON request.</p> <p>When there is only one named input, specify the value of instances key to be the value of the input:</p> <pre><code>{\n  // List of 3 scalar tensors.\n  \"instances\": [ \"foo\", \"bar\", \"baz\" ]\n}\n\n{\n  // List of 2 tensors each of [1, 2] shape\n  \"instances\": [ [[1, 2]], [[3, 4]] ]\n}\n</code></pre> <p>Tensors are expressed naturally in nested notation since there is no need to manually flatten the list.</p> <p>For multiple named inputs, each item is expected to be an object containing input name/tensor value pair, one for each named input. As an example, the following is a request with two instances, each with a set of three named input tensors:</p> <pre><code>{\n \"instances\": [\n   {\n     \"tag\": \"foo\",\n     \"signal\": [1, 2, 3, 4, 5],\n     \"sensor\": [[1, 2], [3, 4]]\n   },\n   {\n     \"tag\": \"bar\",\n     \"signal\": [3, 4, 1, 2, 5]],\n     \"sensor\": [[4, 5], [6, 8]]\n   }\n ]\n}\n</code></pre> <p>Note, each named input (\"tag\", \"signal\", \"sensor\") is implicitly assumed have same 0-th dimension (two in above example, as there are two objects in the instances list). If you have named inputs that have different 0-th dimension, use the columnar format described below.</p>"},{"location":"api/api_rest/#specifying-input-tensors-in-column-format","title":"Specifying input tensors in column format.","text":"<p>Use this format to specify your input tensors, if individual named inputs do not have the same 0-th dimension or you want a more compact representation. This format is similar to the <code>inputs</code> field of the gRPC <code>Predict</code> request.</p> <p>In the columnar format, inputs are keyed to inputs key in the JSON request.</p> <p>The value for inputs key can either a single input tensor or a map of input name to tensors (listed in their natural nested form). Each input can have arbitrary shape and need not share the/ same 0-th dimension (aka batch size) as required by the row format described above.</p> <p>Columnar representation of the previous example is as follows:</p> <pre><code>{\n \"inputs\": {\n   \"tag\": [\"foo\", \"bar\"],\n   \"signal\": [[1, 2, 3, 4, 5], [3, 4, 1, 2, 5]],\n   \"sensor\": [[[1, 2], [3, 4]], [[4, 5], [6, 8]]]\n }\n}\n</code></pre> <p>Note, inputs is a JSON object and not a list like instances (used in the row representation). Also, all the named inputs are specified together, as opposed to unrolling them into individual rows done in the row format described previously. This makes the representation compact (but maybe less readable).</p>"},{"location":"api/api_rest/#response-format_3","title":"Response format","text":"<p>The <code>predict</code> request returns a JSON object in response body.</p> <p>A request in row format has response formatted as follows:</p> <pre><code>{\n  \"predictions\": &lt;value&gt;|&lt;(nested)list&gt;|&lt;list-of-objects&gt;\n}\n</code></pre> <p>If the output of the model contains only one named tensor, we omit the name and <code>predictions</code> key maps to a list of scalar or list values. If the model outputs multiple named tensors, we output a list of objects instead, similar to the request in row-format mentioned above.</p> <p>A request in columnar format has response formatted as follows:</p> <pre><code>{\n  \"outputs\": &lt;value&gt;|&lt;(nested)list&gt;|&lt;object&gt;\n}\n</code></pre> <p>If the output of the model contains only one named tensor, we omit the name and <code>outputs</code> key maps to a list of scalar or list values. If the model outputs multiple named tensors, we output an object instead. Each key of this object corresponds to a named output tensor. The format is similar to the request in column format mentioned above.</p>"},{"location":"api/api_rest/#output-of-binary-values","title":"Output of binary values","text":"<p>TensorFlow does not distinguish between non-binary and binary strings. All are <code>DT_STRING</code> type. Named tensors that have <code>_bytes</code> as a suffix in their name are considered to have binary values. Such values are encoded differently as described in the encoding binary values section below.</p>"},{"location":"api/api_rest/#json-mapping","title":"JSON mapping","text":"<p>The RESTful APIs support a canonical encoding in JSON, making it easier to share data between systems. For supported types, the encodings are described on a type-by-type basis in the table below. Types not listed below are implied to be unsupported.</p> TF Data Type JSON Value JSON example Notes DT_BOOL true, false true, false DT_STRING string \"Hello World!\" If <code>DT_STRING</code> represents binary bytes (e.g. serialized image bytes or protobuf), encode these in Base64. See Encoding binary values for more info. DT_INT8, DT_UINT8, DT_INT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64 number 1, -10, 0 JSON value will be a decimal number. DT_FLOAT, DT_DOUBLE number 1.1, -10.0, 0, <code>NaN</code>, <code>Infinity</code> JSON value will be a number or one of the special token values - <code>NaN</code>, <code>Infinity</code>, and <code>-Infinity</code>. See JSON conformance for more info. Exponent notation is also accepted."},{"location":"api/api_rest/#floating-point-precision","title":"Floating Point Precision","text":"<p>JSON has a single number data type. Thus it is possible to provide a value for an input that results in a loss of precision. For instance, if the input <code>x</code> is a <code>float</code> data type, and the input <code>{\"x\": 1435774380}</code> is sent to the model running on hardware based on the IEEE 754 floating point standard (e.g. Intel or AMD), then the value will be silently converted by the underyling hardware to <code>1435774336</code> since <code>1435774380</code> cannot be exactly represented in a 32-bit floating point number. Typically, the inputs to serving should be the same distribution as training, so this generally won't be problematic because the same conversions happened at training time. However, in case full precision is needed, be sure to use an underlying data type in your model that can handle the desired precision and/or consider client-side checking.</p>"},{"location":"api/api_rest/#encoding-binary-values","title":"Encoding binary values","text":"<p>JSON uses UTF-8 encoding. If you have input feature or tensor values that need to be binary (like image bytes), you must Base64 encode the data and encapsulate it in a JSON object having <code>b64</code> as the key as follows:</p> <pre><code>{ \"b64\": &lt;base64 encoded string&gt; }\n</code></pre> <p>You can specify this object as a value for an input feature or tensor. The same format is used to encode output response as well.</p> <p>A classification request with <code>image</code> (binary data) and <code>caption</code> features is shown below:</p> <pre><code>{\n  \"signature_name\": \"classify_objects\",\n  \"examples\": [\n    {\n      \"image\": { \"b64\": \"aW1hZ2UgYnl0ZXM=\" },\n      \"caption\": \"seaside\"\n    },\n    {\n      \"image\": { \"b64\": \"YXdlc29tZSBpbWFnZSBieXRlcw==\" },\n      \"caption\": \"mountains\"\n    }\n  ]\n}\n</code></pre>"},{"location":"api/api_rest/#json-conformance","title":"JSON conformance","text":"<p>Many feature or tensor values are floating point numbers. Apart from finite values (e.g. 3.14, 1.0 etc.) these can have <code>NaN</code> and non-finite (<code>Infinity</code> and <code>-Infinity</code>) values. Unfortunately the JSON specification (RFC 7159) does NOT recognize these values (though the JavaScript specification does).</p> <p>The REST API described on this page allows request/response JSON objects to have such values. This implies that requests like the following one are valid:</p> <pre><code>{\n  \"example\": [\n    {\n      \"sensor_readings\": [ 1.0, -3.14, Nan, Infinity ]\n    }\n  ]\n}\n</code></pre> <p>A (strict) standards compliant JSON parser will reject this with a parse error (due to <code>NaN</code> and <code>Infinity</code> tokens mixed with actual numbers). To correctly handle requests/responses in your code, use a JSON parser that supports these tokens.</p> <p><code>NaN</code>, <code>Infinity</code>, <code>-Infinity</code> tokens are recognized by proto3, Python JSON module and JavaScript language.</p>"},{"location":"api/api_rest/#example","title":"Example","text":"<p>We can use the toy half_plus_three model to see REST APIs in action.</p>"},{"location":"api/api_rest/#start-modelserver-with-the-rest-api-endpoint","title":"Start ModelServer with the REST API endpoint","text":"<p>Download the <code>half_plus_three</code> model from git repository:</p> <pre><code>$ mkdir -p /tmp/tfserving\n$ cd /tmp/tfserving\n$ git clone --depth=1 https://github.com/tensorflow/serving\n</code></pre> <p>We will use Docker to run the ModelServer. If you want to install ModelServer natively on your system, follow setup instructions to install instead, and start the ModelServer with <code>--rest_api_port</code> option to export REST API endpoint (this is not needed when using Docker).</p> <pre><code>$ cd /tmp/tfserving\n$ docker pull tensorflow/serving:latest\n$ docker run --rm -p 8501:8501 \\\n    --mount type=bind,source=$(pwd),target=$(pwd) \\\n    -e MODEL_BASE_PATH=$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata \\\n    -e MODEL_NAME=saved_model_half_plus_three -t tensorflow/serving:latest\n...\n.... Exporting HTTP/REST API at:localhost:8501 ...\n</code></pre>"},{"location":"api/api_rest/#make-rest-api-calls-to-modelserver","title":"Make REST API calls to ModelServer","text":"<p>In a different terminal, use the <code>curl</code> tool to make REST API calls.</p> <p>Get status of the model as follows:</p> <pre><code>$ curl http://localhost:8501/v1/models/saved_model_half_plus_three\n{\n \"model_version_status\": [\n  {\n   \"version\": \"123\",\n   \"state\": \"AVAILABLE\",\n   \"status\": {\n    \"error_code\": \"OK\",\n    \"error_message\": \"\"\n   }\n  }\n ]\n}\n</code></pre> <p>A <code>predict</code> call would look as follows:</p> <pre><code>$ curl -d '{\"instances\": [1.0,2.0,5.0]}' -X POST http://localhost:8501/v1/models/saved_model_half_plus_three:predict\n{\n    \"predictions\": [3.5, 4.0, 5.5]\n}\n</code></pre> <p>And a <code>regress</code> call looks as follows:</p> <pre><code>$ curl -d '{\"signature_name\": \"tensorflow/serving/regress\", \"examples\": [{\"x\": 1.0}, {\"x\": 2.0}]}' \\\n  -X POST http://localhost:8501/v1/models/saved_model_half_plus_three:regress\n{\n    \"results\": [3.5, 4.0]\n}\n</code></pre> <p>Note, <code>regress</code> is available on a non-default signature name and must be specified explicitly. An incorrect request URL or body returns an HTTP error status.</p> <pre><code>$ curl -i -d '{\"instances\": [1.0,5.0]}' -X POST http://localhost:8501/v1/models/half:predict\nHTTP/1.1 404 Not Found\nContent-Type: application/json\nDate: Wed, 06 Jun 2018 23:20:12 GMT\nContent-Length: 65\n\n{ \"error\": \"Servable not found for request: Latest(half)\" }\n$\n</code></pre>"},{"location":"api/tensorflow_serving/","title":"TensorFlow Serving C++ API Reference","text":""},{"location":"guide/architecture/","title":"Architecture","text":"<p>TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. TensorFlow Serving provides out of the box integration with TensorFlow models, but can be easily extended to serve other types of models.</p>"},{"location":"guide/architecture/#key-concepts","title":"Key Concepts","text":"<p>To understand the architecture of TensorFlow Serving, you need to understand the following key concepts:</p>"},{"location":"guide/architecture/#servables","title":"Servables","text":"<p>Servables are the central abstraction in TensorFlow Serving.  Servables are the underlying objects that clients use to perform computation (for example, a lookup or inference).</p> <p>The size and granularity of a Servable is flexible. A single Servable might include anything from a single shard of a lookup table to a single model to a tuple of inference models. Servables can be of any type and interface, enabling flexibility and future improvements such as:</p> <ul> <li>streaming results</li> <li>experimental APIs</li> <li>asynchronous modes of operation</li> </ul> <p>Servables do not manage their own lifecycle.</p> <p>Typical servables include the following:</p> <ul> <li>a TensorFlow SavedModelBundle (<code>tensorflow::Session</code>)</li> <li>a lookup table for embedding or vocabulary lookups</li> </ul>"},{"location":"guide/architecture/#servable-versions","title":"Servable Versions","text":"<p>TensorFlow Serving can handle one or more versions of a servable over the lifetime of a single server instance.  This enables fresh algorithm configurations, weights, and other  data to be loaded over time. Versions enable more than one  version of a servable to be loaded concurrently, supporting gradual  rollout and experimentation. At serving time, clients may request  either the latest version or a specific version id, for a particular model.</p>"},{"location":"guide/architecture/#servable-streams","title":"Servable Streams","text":"<p>A servable stream is the sequence of versions of a servable, sorted by increasing version numbers.</p>"},{"location":"guide/architecture/#models","title":"Models","text":"<p>TensorFlow Serving represents a model as one or more servables. A machine-learned model may include one or more algorithms (including learned weights) and lookup or embedding tables.</p> <p>You can represent a composite model as either of the following:</p> <ul> <li>multiple independent servables</li> <li>single composite servable</li> </ul> <p>A servable may also correspond to a fraction of a model. For example, a large lookup table could be sharded across many TensorFlow Serving instances.</p>"},{"location":"guide/architecture/#loaders","title":"Loaders","text":"<p>Loaders manage a servable's life cycle. The Loader API enables common infrastructure independent from specific learning algorithms, data or product use-cases involved. Specifically, Loaders standardize the APIs for loading and unloading a servable.</p>"},{"location":"guide/architecture/#sources","title":"Sources","text":"<p>Sources are plugin modules that find and provide servables. Each Source provides zero or more servable streams.  For each servable stream, a Source supplies one Loader instance for each version it makes available to be loaded. (A Source is actually chained together with zero or more SourceAdapters, and the last item in the chain emits the Loaders.)</p> <p>TensorFlow Serving\u2019s interface for Sources can discover servables from arbitrary storage systems. TensorFlow Serving includes common reference Source implementations. For example, Sources may access mechanisms such as RPC and can poll a file system.</p> <p>Sources can maintain state that is shared across multiple servables or versions. This is useful for servables that use delta (diff) updates between versions.</p>"},{"location":"guide/architecture/#aspired-versions","title":"Aspired Versions","text":"<p>Aspired versions represent the set of servable versions that should be loaded and ready. Sources communicate this set of servable versions for a single servable stream at a time. When a Source gives a new list of aspired versions to the Manager, it supercedes the previous list for that servable stream. The Manager unloads any previously loaded versions that no longer appear in the list.</p> <p>See the advanced tutorial to see how version loading works in practice.</p>"},{"location":"guide/architecture/#managers","title":"Managers","text":"<p>Managers handle the full lifecycle of Servables, including:</p> <ul> <li>loading Servables</li> <li>serving Servables</li> <li>unloading Servables</li> </ul> <p>Managers listen to Sources and track all versions. The Manager tries to fulfill Sources' requests, but may refuse to load an aspired version if, say, required resources aren't available.  Managers may also postpone an \"unload\". For example, a Manager may wait to unload until a newer version finishes loading, based on a policy to guarantee that at least one version is loaded at all times.</p> <p>TensorFlow Serving Managers provide a simple, narrow interface -- <code>GetServableHandle()</code> -- for clients to access loaded servable instances.</p>"},{"location":"guide/architecture/#core","title":"Core","text":"<p>Using the standard TensorFlow Serving APis, TensorFlow Serving Core manages the following aspects of servables:</p> <ul> <li>lifecycle</li> <li>metrics</li> </ul> <p>TensorFlow Serving Core treats servables and loaders as opaque objects.</p>"},{"location":"guide/architecture/#life-of-a-servable","title":"Life of a Servable","text":"<p>Broadly speaking:</p> <ol> <li>Sources create Loaders for Servable Versions.</li> <li>Loaders are sent as Aspired Versions to the Manager,    which loads and serves them to client requests.</li> </ol> <p>In more detail:</p> <ol> <li>A Source plugin creates a Loader for a specific version. The Loader contains whatever metadata it needs to load the Servable.</li> <li>The Source uses a callback to notify the Manager of the Aspired Version.</li> <li>The Manager applies the configured Version Policy to determine the next action to take, which could be to unload a previously loaded version or to load the new version.</li> <li>If the Manager determines that it's safe, it gives the Loader the required resources and tells the Loader to load the new version.</li> <li>Clients ask the Manager for the Servable, either specifying a version explicitly or just requesting the latest version. The Manager returns a handle for the Servable.</li> </ol> <p>For example, say a Source represents a TensorFlow graph with frequently updated model weights. The weights are stored in a file on disk.</p> <ol> <li>The Source detects a new version of the model weights. It creates a Loader that contains a pointer to the model data on disk.</li> <li>The Source notifies the Dynamic Manager of the Aspired Version.</li> <li>The Dynamic Manager applies the Version Policy and decides to load the new version.</li> <li>The Dynamic Manager tells the Loader that there is enough memory. The Loader instantiates the TensorFlow graph with the new weights.</li> <li>A client requests a handle to the latest version of the model, and the Dynamic Manager returns a handle to the new version of the Servable.</li> </ol>"},{"location":"guide/architecture/#extensibility","title":"Extensibility","text":"<p>TensorFlow Serving provides several extension points where you can add new functionality.</p>"},{"location":"guide/architecture/#version-policy","title":"Version Policy","text":"<p>Version Policies specify the sequence of version loading and unloading within a single servable stream.</p> <p>TensorFlow Serving includes two policies that accommodate most known use- cases. These are the Availability Preserving Policy (avoid leaving zero versions loaded; typically load a new version before unloading an old one), and the Resource Preserving Policy (avoid having two versions loaded simultaneously, thus requiring double the resources; unload an old version before loading a new one). For simple usage of TensorFlow Serving where the serving availability of a model is important and the resource costs low, the Availability Preserving Policy will ensure that the new version is loaded and ready before unloading the old one. For sophisticated usage of TensorFlow Serving, for example managing versions across multiple server instances, the Resource Preserving Policy requires the least resources (no extra buffer for loading new versions).</p>"},{"location":"guide/architecture/#source","title":"Source","text":"<p>New Sources could support new filesystems, cloud offerings and algorithm backends. TensorFlow Serving provides some common building blocks to make it easy &amp; fast to create new sources. For example, TensorFlow Serving includes a utility to wrap polling behavior around a simple source. Sources are closely related to Loaders for specific algorithms and data hosting servables.</p> <p>See the Custom Source document for more about how to create a custom Source.</p>"},{"location":"guide/architecture/#loaders_1","title":"Loaders","text":"<p>Loaders are the extension point for adding algorithm and data backends. TensorFlow is one such algorithm backend. For example, you would implement a new Loader in order to load, provide access to, and unload an instance of a new type of servable machine learning model. We anticipate creating Loaders for lookup tables and additional algorithms.</p> <p>See the Custom Servable document to learn how to create a custom servable.</p>"},{"location":"guide/architecture/#batcher","title":"Batcher","text":"<p>Batching of multiple requests into a single request can significantly reduce the cost of performing inference, especially in the presence of hardware accelerators such as GPUs. TensorFlow Serving includes a request batching widget that lets clients easily batch their type-specific inferences across requests into batch requests that algorithm systems can more efficiently process. See the Batching Guide for more information.</p>"},{"location":"guide/custom_op/","title":"Serving TensorFlow models with custom ops","text":"<p>TensorFlow comes pre-built with an extensive library of ops and op kernels (implementations) fine-tuned for different hardware types (CPU, GPU, etc.). These operations are automatically linked into the TensorFlow Serving ModelServer binary with no additional work required by the user. However, there are two use cases that require the user to link in ops into the ModelServer explicitly:</p> <ul> <li>You have written your own custom op (ex. using     this guide)</li> <li>You are using an already implemented op that is not shipped with TensorFlow</li> </ul> <p>Note</p> <p>Starting in version 2.0, TensorFlow no longer distributes the contrib module; if you are serving a TensorFlow program using contrib ops, use this guide to link these ops into ModelServer explicitly.</p> <p>Regardless of whether you implemented the op or not, in order to serve a model with custom ops, you need access to the source of the op. This guide walks you through the steps of using the source to make custom ops available for serving. For guidance on implementation of custom ops, please refer to the tensorflow/custom-op repo.</p> <p>Prerequisite: With Docker installed, you have cloned the TensorFlow Serving repository and your current working directory is the root of the repo.</p>"},{"location":"guide/custom_op/#copy-over-op-source-into-serving-project","title":"Copy over op source into Serving project","text":"<p>In order to build TensorFlow Serving with your custom ops, you will first need to copy over the op source into your serving project. For this example, you will use tensorflow_zero_out from the custom-op repository mentioned above.</p> <p>Wihin the serving repo, create a <code>custom_ops</code> directory, which will house all your custom ops. For this example, you will only have the tensorflow_zero_out code.</p> <pre><code>mkdir tensorflow_serving/custom_ops\ncp -r &lt;custom_ops_repo_root&gt;/tensorflow_zero_out tensorflow_serving/custom_ops\n</code></pre>"},{"location":"guide/custom_op/#build-static-library-for-the-op","title":"Build static library for the op","text":"<p>In tensorflow_zero_out's BUILD file, you see a target producing a shared object file (<code>.so</code>), which you would load into python in order to create and train your model. TensorFlow Serving, however, statically links ops at build time, and requires a <code>.a</code> file. So you will add a build rule that produces this file to <code>tensorflow_serving/custom_ops/tensorflow_zero_out/BUILD</code>:</p> <pre><code>cc_library(\n    name = 'zero_out_ops',\n    srcs = [\n        \"cc/kernels/zero_out_kernels.cc\",\n        \"cc/ops/zero_out_ops.cc\",\n    ],\n    alwayslink = 1,\n    deps = [\n        \"@org_tensorflow//tensorflow/core:framework\",\n    ]\n)\n</code></pre>"},{"location":"guide/custom_op/#build-modelserver-with-the-op-linked-in","title":"Build ModelServer with the op linked in","text":"<p>To serve a model that uses a custom op, you have to build the ModelServer binary with that op linked in. Specifically, you add the <code>zero_out_ops</code> build target created above to the ModelServer's <code>BUILD</code> file.</p> <p>Edit <code>tensorflow_serving/model_servers/BUILD</code> to add your custom op build target to <code>SUPPORTED_TENSORFLOW_OPS</code> which is inluded in the <code>server_lib</code> target:</p> <pre><code>SUPPORTED_TENSORFLOW_OPS = [\n    ...\n    \"//tensorflow_serving/custom_ops/tensorflow_zero_out:zero_out_ops\"\n]\n</code></pre> <p>Then use the Docker environment to build the ModelServer:</p> <pre><code>tools/run_in_docker.sh bazel build tensorflow_serving/model_servers:tensorflow_model_server\n</code></pre>"},{"location":"guide/custom_op/#serve-a-model-containing-your-custom-op","title":"Serve a model containing your custom op","text":"<p>You can now run the ModelServer binary and start serving a model that contains this custom op:</p> <pre><code>tools/run_in_docker.sh -o \"-p 8501:8501\" \\\nbazel-bin/tensorflow_serving/model_servers/tensorflow_model_server \\\n--rest_api_port=8501 --model_name=&lt;model_name&gt; --model_base_path=&lt;model_base_path&gt;\n</code></pre>"},{"location":"guide/custom_op/#send-an-inference-request-to-test-op-manually","title":"Send an inference request to test op manually","text":"<p>You can now send an inference request to the model server to test your custom op:</p> <pre><code>curl http://localhost:8501/v1/models/&lt;model_name&gt;:predict -X POST \\\n-d '{\"inputs\": [[1,2], [3,4]]}'\n</code></pre> <p>This page contains a more complete API for sending REST requests to the model server.</p>"},{"location":"guide/custom_servable/","title":"Creating a new kind of servable","text":"<p>This document explains how to extend TensorFlow Serving with a new kind of servable. The most prominent servable type is <code>SavedModelBundle</code>, but it can be useful to define other kinds of servables, to serve data that goes along with your model. Examples include: a vocabulary lookup table, feature transformation logic. Any C++ class can be a servable, e.g. <code>int</code>, <code>std::map&lt;string, int&gt;</code> or any class defined in your binary -- let us call it <code>YourServable</code>.</p>"},{"location":"guide/custom_servable/#defining-a-loader-and-sourceadapter-for-yourservable","title":"Defining a <code>Loader</code> and <code>SourceAdapter</code> for <code>YourServable</code>","text":"<p>To enable TensorFlow Serving to manage and serve <code>YourServable</code>, you need to define two things:</p> <ol> <li> <p>A <code>Loader</code> class that loads, provides access to, and unloads an instance   of <code>YourServable</code>.</p> </li> <li> <p>A <code>SourceAdapter</code> that instantiates loaders from some underlying data   format e.g. file-system paths. As an alternative to a <code>SourceAdapter</code>, you   could write a complete <code>Source</code>. However, since the <code>SourceAdapter</code>   approach is more common and more modular, we focus on it here.</p> </li> </ol> <p>The <code>Loader</code> abstraction is defined in <code>core/loader.h</code>. It requires you to define methods for loading, accessing and unloading your type of servable. The data from which the servable is loaded can come from anywhere, but it is common for it to come from a storage-system path. Let us assume that is the case for <code>YourServable</code>. Let us further assume you already have a <code>Source&lt;StoragePath&gt;</code> that you are happy with (if not, see the Custom Source document).</p> <p>In addition to your <code>Loader</code>, you will need to define a <code>SourceAdapter</code> that instantiates a <code>Loader</code> from a given storage path. Most simple use-cases can specify the two objects concisely with the <code>SimpleLoaderSourceAdapter</code> class (in <code>core/simple_loader.h</code>). Advanced use-cases may opt to specify <code>Loader</code> and <code>SourceAdapter</code> classes separately using the lower-level APIs, e.g. if the <code>SourceAdapter</code> needs to retain some state, and/or if state needs to be shared among <code>Loader</code> instances.</p> <p>There is a reference implementation of a simple hashmap servable that uses <code>SimpleLoaderSourceAdapter</code> in <code>servables/hashmap/hashmap_source_adapter.cc</code>. You may find it convenient to make a copy of <code>HashmapSourceAdapter</code> and then modify it to suit your needs.</p> <p>The implementation of <code>HashmapSourceAdapter</code> has two parts:</p> <ol> <li> <p>The logic to load a hashmap from a file, in <code>LoadHashmapFromFile()</code>.</p> </li> <li> <p>The use of <code>SimpleLoaderSourceAdapter</code> to define a <code>SourceAdapter</code> that   emits hashmap loaders based on <code>LoadHashmapFromFile()</code>. The new   <code>SourceAdapter</code> can be instantiated from a configuration protocol message of   type <code>HashmapSourceAdapterConfig</code>. Currently, the configuration message   contains just the file format, and for the purpose of the reference   implementation just a single simple format is supported.</p> </li> </ol> <p>Note the call to <code>Detach()</code> in the destructor. This call is required to avoid   races between tearing down state and any ongoing invocations of the Creator   lambda in other threads. (Even though this simple source adapter doesn't have   any state, the base class nevertheless enforces that Detach() gets called.)</p>"},{"location":"guide/custom_servable/#arranging-for-yourservable-objects-to-be-loaded-in-a-manager","title":"Arranging for <code>YourServable</code> objects to be loaded in a manager","text":"<p>Here is how to hook your new <code>SourceAdapter</code> for <code>YourServable</code> loaders to a basic source of storage paths, and a manager (with bad error handling; real code should be more careful):</p> <p>First, create a manager:</p> <pre><code>std::unique_ptr&lt;AspiredVersionsManager&gt; manager = ...;\n</code></pre> <p>Then, create a <code>YourServable</code> source adapter and plug it into the manager:</p> <pre><code>auto your_adapter = new YourServableSourceAdapter(...);\nConnectSourceToTarget(your_adapter, manager.get());\n</code></pre> <p>Lastly, create a simple path source and plug it into your adapter:</p> <pre><code>std::unique_ptr&lt;FileSystemStoragePathSource&gt; path_source;\n// Here are some FileSystemStoragePathSource config settings that ought to get\n// it working, but for details please see its documentation.\nFileSystemStoragePathSourceConfig config;\n// We just have a single servable stream. Call it \"default\".\nconfig.set_servable_name(\"default\");\nconfig.set_base_path(FLAGS::base_path /* base path for our servable files */);\nconfig.set_file_system_poll_wait_seconds(1);\nTF_CHECK_OK(FileSystemStoragePathSource::Create(config, &amp;path_source));\nConnectSourceToTarget(path_source.get(), your_adapter.get());\n</code></pre>"},{"location":"guide/custom_servable/#accessing-loaded-yourservable-objects","title":"Accessing loaded <code>YourServable</code> objects","text":"<p>Here is how to get a handle to a loaded <code>YourServable</code>, and use it:</p> <pre><code>auto handle_request = serving::ServableRequest::Latest(\"default\");\nServableHandle&lt;YourServable*&gt; servable;\nStatus status = manager-&gt;GetServableHandle(handle_request, &amp;servable);\nif (!status.ok()) {\n  LOG(INFO) &lt;&lt; \"Zero versions of 'default' servable have been loaded so far\";\n  return;\n}\n// Use the servable.\n(*servable)-&gt;SomeYourServableMethod();\n</code></pre>"},{"location":"guide/custom_servable/#advanced-arranging-for-multiple-servable-instances-to-share-state","title":"Advanced: Arranging for multiple servable instances to share state","text":"<p>SourceAdapters can house state that is shared among multiple emitted servables. For example:</p> <ul> <li> <p>A shared thread pool or other resource that multiple servables use.</p> </li> <li> <p>A shared read-only data structure that multiple servables use, to avoid the   time and space overhead of replicating the data structure in each servable   instance.</p> </li> </ul> <p>Shared state whose initialization time and size is negligible (e.g. thread pools) can be created eagerly by the SourceAdapter, which then embeds a pointer to it in each emitted servable loader. Creation of expensive or large shared state should be deferred to the first applicable Loader::Load() call, i.e. governed by the manager. Symmetrically, the Loader::Unload() call to the final servable using the expensive/large shared state should tear it down.</p>"},{"location":"guide/custom_source/","title":"Creating a module that discovers new servable paths","text":"<p>This document explains how to extend TensorFlow Serving to monitor different storage systems to discover new (versions of) models or data to serve. In particular, it covers how to create and use a module that monitors a storage system path for the appearance of new sub-paths, where each sub-path represents a new servable version to load. That kind of module is called a <code>Source&lt;StoragePath&gt;</code>, because it emits objects of type <code>StoragePath</code> (typedefed to <code>string</code>). It can be composed with a <code>SourceAdapter</code> that creates a servable <code>Loader</code> from a given path that the source discovers.</p>"},{"location":"guide/custom_source/#first-a-note-about-generality","title":"First, a note about generality","text":"<p>Using paths as handles to servable data is not required; it merely illustrates one way to ingest servables into the system. Even if your environment does not encapsulate servable data in paths, this document will familiarize you with the key abstractions. You have the option to create <code>Source&lt;T&gt;</code> and <code>SourceAdapter&lt;T1, T2&gt;</code> modules for types that suit your environment (e.g. RPC or pub/sub messages, database records), or to simply create a monolithic <code>Source&lt;std::unique_ptr&lt;Loader&gt;&gt;</code> that emits servable loaders directly.</p> <p>Of course, whatever kind of data your source emits (whether it is POSIX paths, Google Cloud Storage paths, or RPC handles), there needs to be accompanying module(s) that are able to load servables based on that. Such modules are called <code>SourceAdapters</code>. Creating a custom one is described in the Custom Servable document. TensorFlow Serving comes with one for instantiating TensorFlow sessions based on paths in file systems that TensorFlow supports. One can add support for additional file systems to TensorFlow by extending the <code>RandomAccessFile</code> abstraction (<code>tensorflow/core/public/env.h</code>).</p> <p>This document focuses on creating a source that emits paths in a TensorFlow-supported file system. It ends with a walk-through of how to use your source in conjunction with pre-existing modules to serve TensorFlow models.</p>"},{"location":"guide/custom_source/#creating-your-source","title":"Creating your Source","text":"<p>We have a reference implementation of a <code>Source&lt;StoragePath&gt;</code>, called <code>FileSystemStoragePathSource</code> (at <code>sources/storage_path/file_system_storage_path_source*</code>). <code>FileSystemStoragePathSource</code> monitors a particular file system path, watches for numerical sub-directories, and reports the latest of these as the version it aspires to load. This document walks through the salient aspects of <code>FileSystemStoragePathSource</code>. You may find it convenient to make a copy of <code>FileSystemStoragePathSource</code> and then modify it to suit your needs.</p> <p>First, <code>FileSystemStoragePathSource</code> implements the <code>Source&lt;StoragePath&gt;</code> API, which is a specialization of the <code>Source&lt;T&gt;</code> API with <code>T</code> bound to <code>StoragePath</code>. The API consists of a single method <code>SetAspiredVersionsCallback()</code>, which supplies a closure the source can invoke to communicate that it wants a particular set of servable versions to be loaded.</p> <p><code>FileSystemStoragePathSource</code> uses the aspired-versions callback in a very simple way: it periodically inspects the file system (doing an <code>ls</code>, essentially), and if it finds one or more paths that look like servable versions it determines which one is the latest version and invokes the callback with a list of size one containing just that version (under the default configuration). So, at any given time <code>FileSystemStoragePathSource</code> requests at most one servable to be loaded, and its implementation takes advantage of the idempotence of the callback to keep itself stateless (there is no harm in invoking the callback repeatedly with the same arguments).</p> <p><code>FileSystemStoragePathSource</code> has a static initialization factory (the <code>Create()</code> method), which takes a configuration protocol message. The configuration message includes details such as the base path to monitor and the monitoring interval. It also includes the name of the servable stream to emit. (Alternative approaches might extract the servable stream name from the base path, to emit multiple servable streams based on observing a deeper directory hierarchy; those variants are beyond the scope of the reference implementation.)</p> <p>The bulk of the implementation consists of a thread that periodically examines the file system, along with some logic for identifying and sorting any numerical sub-paths it discovers. The thread is launched inside <code>SetAspiredVersionsCallback()</code> (not in <code>Create()</code>) because that is the point at which the source should \"start\" and knows where to send aspired-version requests.</p>"},{"location":"guide/custom_source/#using-your-source-to-load-tensorflow-sessions","title":"Using your Source to load TensorFlow sessions","text":"<p>You will likely want to use your new source module in conjunction with <code>SavedModelBundleSourceAdapter</code> (<code>servables/tensorflow/saved_model_bundle_source_adapter*</code>), which will interpret each path your source emits as a TensorFlow export, and convert each path to a loader for a TensorFlow <code>SavedModelBundle</code> servable. You will likely plug the <code>SavedModelBundle</code> adapter into a <code>AspiredVersionsManager</code>, which takes care of actually loading and serving the servables. A good illustration of chaining these three kinds of modules together to get a working server library is found in <code>servables/tensorflow/simple_servers.cc</code>. Here is a walk-through of the main code flow (with bad error handling; real code should be more careful):</p> <p>First, create a manager:</p> <pre><code>std::unique_ptr&lt;AspiredVersionsManager&gt; manager = ...;\n</code></pre> <p>Then, create a <code>SavedModelBundle</code> source adapter and plug it into the manager:</p> <pre><code>std::unique_ptr&lt;SavedModelBundleSourceAdapter&gt; bundle_adapter;\nSavedModelBundleSourceAdapterConfig config;\n// ... populate 'config' with TensorFlow options.\nTF_CHECK_OK(SavedModelBundleSourceAdapter::Create(config, &amp;bundle_adapter));\nConnectSourceToTarget(bundle_adapter.get(), manager.get());\n</code></pre> <p>Lastly, create your path source and plug it into the <code>SavedModelBundle</code> adapter:</p> <pre><code>auto your_source = new YourPathSource(...);\nConnectSourceToTarget(your_source, bundle_adapter.get());\n</code></pre> <p>The <code>ConnectSourceToTarget()</code> function (defined in <code>core/target.h</code>) merely invokes <code>SetAspiredVersionsCallback()</code> to connect a <code>Source&lt;T&gt;</code> to a <code>Target&lt;T&gt;</code> (a <code>Target</code> is a module that catches aspired-version requests, i.e. an adapter or manager).</p>"},{"location":"guide/docker/","title":"TensorFlow Serving with Docker","text":"<p>One of the easiest ways to get started using TensorFlow Serving is with Docker.</p> <pre><code># Download the TensorFlow Serving Docker image and repo\ndocker pull tensorflow/serving\ngit clone https://github.com/tensorflow/serving\n# Location of demo models\nTESTDATA=\"$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata\"\n\n# Start TensorFlow Serving container and open the REST API port\ndocker run -t --rm -p 8501:8501 \\\n    -v \"$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two\" \\\n    -e MODEL_NAME=half_plus_two \\\n    tensorflow/serving &amp;\n\n# Query the model using the predict API\ncurl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n    -X POST http://localhost:8501/v1/models/half_plus_two:predict\n# Returns =&gt; { \"predictions\": [2.5, 3.0, 4.5] }\n</code></pre> <p>For additional serving endpoints, see the Client REST API.</p>"},{"location":"guide/docker/#install-docker","title":"Install Docker","text":"<p>General installation instructions are on the Docker site, but we give some quick links here:</p> <ul> <li>Docker for macOS</li> <li>Docker for Windows     for Windows 10 Pro or later</li> <li>Docker Toolbox for much older versions     of macOS, or versions of Windows before Windows 10 Pro</li> </ul>"},{"location":"guide/docker/#serving-with-docker","title":"Serving with Docker","text":""},{"location":"guide/docker/#pulling-a-serving-image","title":"Pulling a serving image","text":"<p>Once you have Docker installed, you can pull the latest TensorFlow Serving docker image by running:</p> <pre><code>docker pull tensorflow/serving\n</code></pre> <p>This will pull down a minimal Docker image with TensorFlow Serving installed.</p> <p>See the Docker Hub tensorflow/serving repo for other versions of images you can pull.</p>"},{"location":"guide/docker/#running-a-serving-image","title":"Running a serving image","text":"<p>The serving images (both CPU and GPU) have the following properties:</p> <ul> <li>Port 8500 exposed for gRPC</li> <li>Port 8501 exposed for the REST API</li> <li>Optional environment variable <code>MODEL_NAME</code> (defaults to <code>model</code>)</li> <li>Optional environment variable <code>MODEL_BASE_PATH</code> (defaults to <code>/models</code>)</li> </ul> <p>When the serving image runs ModelServer, it runs it as follows:</p> <pre><code>tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n  --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME}\n</code></pre> <p>To serve with Docker, you'll need:</p> <ul> <li>An open port on your host to serve on</li> <li>A SavedModel to serve</li> <li>A name for your model that your client will refer to</li> </ul> <p>What you'll do is run the Docker container, publish the container's ports to your host's ports, and mounting your host's path to the SavedModel to where the container expects models.</p> <p>Let's look at an example:</p> <pre><code>docker run -p 8501:8501 \\\n  --mount type=bind,source=/path/to/my_model/,target=/models/my_model \\\n  -e MODEL_NAME=my_model -t tensorflow/serving\n</code></pre> <p>In this case, we've started a Docker container, published the REST API port 8501 to our host's port 8501, and taken a model we named <code>my_model</code> and bound it to the default model base path (<code>${MODEL_BASE_PATH}/${MODEL_NAME}</code> = <code>/models/my_model</code>). Finally, we've filled in the environment variable <code>MODEL_NAME</code> with <code>my_model</code>, and left <code>MODEL_BASE_PATH</code> to its default value.</p> <p>This will run in the container:</p> <pre><code>tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n  --model_name=my_model --model_base_path=/models/my_model\n</code></pre> <p>If we wanted to publish the gRPC port, we would use <code>-p 8500:8500</code>. You can have both gRPC and REST API ports open at the same time, or choose to only open one or the other.</p>"},{"location":"guide/docker/#passing-additional-arguments","title":"Passing additional arguments","text":"<p><code>tensorflow_model_server</code> supports many additional arguments that you could pass to the serving docker containers. For example, if we wanted to pass a model config file instead of specifying the model name, we could do the following:</p> <pre><code>docker run -p 8500:8500 -p 8501:8501 \\\n  --mount type=bind,source=/path/to/my_model/,target=/models/my_model \\\n  --mount type=bind,source=/path/to/my/models.config,target=/models/models.config \\\n  -t tensorflow/serving --model_config_file=/models/models.config\n</code></pre> <p>This approach works for any of the other command line arguments that <code>tensorflow_model_server</code> supports.</p>"},{"location":"guide/docker/#creating-your-own-serving-image","title":"Creating your own serving image","text":"<p>If you want a serving image that has your model built into the container, you can create your own image.</p> <p>First run a serving image as a daemon:</p> <pre><code>docker run -d --name serving_base tensorflow/serving\n</code></pre> <p>Next, copy your SavedModel to the container's model folder:</p> <pre><code>docker cp models/&lt;my model&gt; serving_base:/models/&lt;my model&gt;\n</code></pre> <p>Finally, commit the container that's serving your model by changing <code>MODEL_NAME</code> to match your model's name <code>&lt;my model&gt;</code>:</p> <pre><code>docker commit --change \"ENV MODEL_NAME &lt;my model&gt;\" serving_base &lt;my container&gt;\n</code></pre> <p>You can now stop <code>serving_base</code></p> <pre><code>docker kill serving_base\n</code></pre> <p>This will leave you with a Docker image called <code>&lt;my container&gt;</code> that you can deploy and will load your model for serving on startup.</p>"},{"location":"guide/docker/#serving-example","title":"Serving example","text":"<p>Let's run through a full example where we load a SavedModel and call it using the REST API. First pull the serving image:</p> <pre><code>docker pull tensorflow/serving\n</code></pre> <p>This will pull the latest TensorFlow Serving image with ModelServer installed.</p> <p>Next, we will use a toy model called <code>Half Plus Two</code>, which generates <code>0.5 * x + 2</code> for the values of <code>x</code> we provide for prediction.</p> <p>To get this model, first clone the TensorFlow Serving repo.</p> <pre><code>mkdir -p /tmp/tfserving\ncd /tmp/tfserving\ngit clone https://github.com/tensorflow/serving\n</code></pre> <p>Next, run the TensorFlow Serving container pointing it to this model and opening the REST API port (8501):</p> <pre><code>docker run -p 8501:8501 \\\n  --mount type=bind,\\\nsource=/tmp/tfserving/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu,\\\ntarget=/models/half_plus_two \\\n  -e MODEL_NAME=half_plus_two -t tensorflow/serving &amp;\n</code></pre> <p>This will run the docker container and launch the TensorFlow Serving Model Server, bind the REST API port 8501, and map our desired model from our host to where models are expected in the container. We also pass the name of the model as an environment variable, which will be important when we query the model.</p> <p>To query the model using the predict API, you can run</p> <pre><code>curl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n  -X POST http://localhost:8501/v1/models/half_plus_two:predict\n</code></pre> <p>Note</p> <p>Older versions of Windows and other systems without curl can download it here.</p> <p>This should return a set of values:</p> <pre><code>{ \"predictions\": [2.5, 3.0, 4.5] }\n</code></pre> <p>More information on using the RESTful API can be found here.</p>"},{"location":"guide/docker/#serving-with-docker-using-your-gpu","title":"Serving with Docker using your GPU","text":""},{"location":"guide/docker/#install-nvidia-docker","title":"Install nvidia-docker","text":"<p>Before serving with a GPU, in addition to installing Docker, you will need:</p> <ul> <li>Up-to-date NVIDIA drivers for your system</li> <li><code>nvidia-docker</code>: You can follow the     installation instructions here</li> </ul>"},{"location":"guide/docker/#running-a-gpu-serving-image","title":"Running a GPU serving image","text":"<p>Running a GPU serving image is identical to running a CPU image. For more details, see running a serving image.</p>"},{"location":"guide/docker/#gpu-serving-example","title":"GPU Serving example","text":"<p>Let's run through a full example where we load a model with GPU-bound ops and call it using the REST API.</p> <p>First install <code>nvidia-docker</code>. Next you can pull the latest TensorFlow Serving GPU docker image by running:</p> <pre><code>docker pull tensorflow/serving:latest-gpu\n</code></pre> <p>This will pull down an minimal Docker image with ModelServer built for running on GPUs installed.</p> <p>Next, we will use a toy model called <code>Half Plus Two</code>, which generates <code>0.5 * x + 2</code> for the values of <code>x</code> we provide for prediction. This model will have ops bound to the GPU device, and will not run on the CPU.</p> <p>To get this model, first clone the TensorFlow Serving repo.</p> <pre><code>mkdir -p /tmp/tfserving\ncd /tmp/tfserving\ngit clone https://github.com/tensorflow/serving\n</code></pre> <p>Next, run the TensorFlow Serving container pointing it to this model and opening the REST API port (8501):</p> <pre><code>docker run --gpus all -p 8501:8501 \\\n--mount type=bind,\\\nsource=/tmp/tfserving/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_gpu,\\\ntarget=/models/half_plus_two \\\n  -e MODEL_NAME=half_plus_two -t tensorflow/serving:latest-gpu &amp;\n</code></pre> <p>This will run the docker container, launch the TensorFlow Serving Model Server, bind the REST API port 8501, and map our desired model from our host to where models are expected in the container. We also pass the name of the model as an environment variable, which will be important when we query the model.</p> <p>Tip</p> <p>Before querying the model, be sure to wait till you see a message like the following, indicating that the server is ready to receive requests:</p> <pre><code>2018-07-27 00:07:20.773693: I tensorflow_serving/model_servers/main.cc:333]\nExporting HTTP/REST API at:localhost:8501 ...\n</code></pre> <p>To query the model using the predict API, you can run</p> <pre><code>curl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n  -X POST http://localhost:8501/v1/models/half_plus_two:predict\n</code></pre> <p>Note</p> <p>Older versions of Windows and other systems without curl can download it here.</p> <p>This should return a set of values:</p> <pre><code>{ \"predictions\": [2.5, 3.0, 4.5] }\n</code></pre> <p>Tip</p> <p>Trying to run the GPU model on a machine without a GPU or without a working GPU build of TensorFlow Model Server will result in an error that looks like:</p> <pre><code>Cannot assign a device for operation 'a': Operation was explicitly assigned to /device:GPU:0\n</code></pre> <p>More information on using the RESTful API can be found here.</p>"},{"location":"guide/docker/#developing-with-docker","title":"Developing with Docker","text":"<p>For instructions on how to build and develop Tensorflow Serving, please refer to Developing with Docker guide.</p>"},{"location":"guide/serving_advanced/","title":"Building Standard TensorFlow ModelServer","text":"<p>This tutorial shows you how to use TensorFlow Serving components to build the standard TensorFlow ModelServer that dynamically discovers and serves new versions of a trained TensorFlow model. If you just want to use the standard server to serve your models, see TensorFlow Serving basic tutorial.</p> <p>This tutorial uses the simple Softmax Regression model introduced in the TensorFlow tutorial for handwritten image (MNIST data) classification. If you don't know what TensorFlow or MNIST is, see the MNIST For ML Beginners tutorial.</p> <p>The code for this tutorial consists of two parts:</p> <ul> <li> <p>A Python file   mnist_saved_model.py   that trains and exports multiple versions of the model.</p> </li> <li> <p>A C++ file   main.cc   which is the standard TensorFlow ModelServer that discovers new exported   models and runs a gRPC service for serving them.</p> </li> </ul> <p>This tutorial steps through the following tasks:</p> <ol> <li>Train and export a TensorFlow model.</li> <li>Manage model versioning with TensorFlow Serving <code>ServerCore</code>.</li> <li>Configure batching using <code>SavedModelBundleSourceAdapterConfig</code>.</li> <li>Serve request with TensorFlow Serving <code>ServerCore</code>.</li> <li>Run and test the service.</li> </ol> <p>Before getting started, first install Docker</p>"},{"location":"guide/serving_advanced/#train-and-export-tensorflow-model","title":"Train and export TensorFlow Model","text":"<p>First, if you haven't done so yet, clone this repository to your local machine:</p> <pre><code>git clone https://github.com/tensorflow/serving.git\ncd serving\n</code></pre> <p>Clear the export directory if it already exists:</p> <pre><code>rm -rf /tmp/models\n</code></pre> <p>Train (with 100 iterations) and export the first version of model:</p> <pre><code>tools/run_in_docker.sh python tensorflow_serving/example/mnist_saved_model.py \\\n  --training_iteration=100 --model_version=1 /tmp/mnist\n</code></pre> <p>Train (with 2000 iterations) and export the second version of model:</p> <pre><code>tools/run_in_docker.sh python tensorflow_serving/example/mnist_saved_model.py \\\n  --training_iteration=2000 --model_version=2 /tmp/mnist\n</code></pre> <p>As you can see in <code>mnist_saved_model.py</code>, the training and exporting is done the same way it is in the TensorFlow Serving basic tutorial. For demonstration purposes, you're intentionally dialing down the training iterations for the first run and exporting it as v1, while training it normally for the second run and exporting it as v2 to the same parent directory -- as we expect the latter to achieve better classification accuracy due to more intensive training. You should see training data for each training run in your <code>/tmp/mnist</code> directory:</p> <pre><code>$ ls /tmp/mnist\n1  2\n</code></pre>"},{"location":"guide/serving_advanced/#servercore","title":"ServerCore","text":"<p>Now imagine v1 and v2 of the model are dynamically generated at runtime, as new algorithms are being experimented with, or as the model is trained with a new data set. In a production environment, you may want to build a server that can support gradual rollout, in which v2 can be discovered, loaded, experimented, monitored, or reverted while serving v1. Alternatively, you may want to tear down v1 before bringing up v2. TensorFlow Serving supports both options -- while one is good for maintaining availability during the transition, the other is good for minimizing resource usage (e.g. RAM).</p> <p>TensorFlow Serving <code>Manager</code> does exactly that. It handles the full lifecycle of TensorFlow models including loading, serving and unloading them as well as version transitions. In this tutorial, you will build your server on top of a TensorFlow Serving <code>ServerCore</code>, which internally wraps an <code>AspiredVersionsManager</code>.</p> <pre><code>int main(int argc, char** argv) {\n  ...\n\n  ServerCore::Options options;\n  options.model_server_config = model_server_config;\n  options.servable_state_monitor_creator = &amp;CreateServableStateMonitor;\n  options.custom_model_config_loader = &amp;LoadCustomModelConfig;\n\n  ::google::protobuf::Any source_adapter_config;\n  SavedModelBundleSourceAdapterConfig\n      saved_model_bundle_source_adapter_config;\n  source_adapter_config.PackFrom(saved_model_bundle_source_adapter_config);\n  (*(*options.platform_config_map.mutable_platform_configs())\n      [kTensorFlowModelPlatform].mutable_source_adapter_config()) =\n      source_adapter_config;\n\n  std::unique_ptr&lt;ServerCore&gt; core;\n  TF_CHECK_OK(ServerCore::Create(options, &amp;core));\n  RunServer(port, std::move(core));\n\n  return 0;\n}\n</code></pre> <p><code>ServerCore::Create()</code> takes a ServerCore::Options parameter. Here are a few commonly used options:</p> <ul> <li><code>ModelServerConfig</code> that specifies models to be loaded. Models are declared   either through <code>model_config_list</code>, which declares a static list of models, or   through <code>custom_model_config</code>, which defines a custom way to declare a list of   models that may get updated at runtime.</li> <li><code>PlatformConfigMap</code> that maps from the name of the platform (such as   <code>tensorflow</code>) to the <code>PlatformConfig</code>, which is used to create the   <code>SourceAdapter</code>. <code>SourceAdapter</code> adapts <code>StoragePath</code> (the path where a model   version is discovered) to model <code>Loader</code> (loads the model version from   storage path and provides state transition interfaces to the <code>Manager</code>). If   <code>PlatformConfig</code> contains <code>SavedModelBundleSourceAdapterConfig</code>, a   <code>SavedModelBundleSourceAdapter</code> will be created, which we will explain later.</li> </ul> <p><code>SavedModelBundle</code> is a key component of TensorFlow Serving. It represents a TensorFlow model loaded from a given path and provides the same <code>Session::Run</code> interface as TensorFlow to run inference. <code>SavedModelBundleSourceAdapter</code> adapts storage path to <code>Loader&lt;SavedModelBundle&gt;</code> so that model lifetime can be managed by <code>Manager</code>. Please note that <code>SavedModelBundle</code> is the successor of deprecated <code>SessionBundle</code>. Users are encouraged to use <code>SavedModelBundle</code> as the support for <code>SessionBundle</code> will soon be removed.</p> <p>With all these, <code>ServerCore</code> internally does the following:</p> <ul> <li>Instantiates a <code>FileSystemStoragePathSource</code> that monitors model export   paths declared in <code>model_config_list</code>.</li> <li>Instantiates a <code>SourceAdapter</code> using the <code>PlatformConfigMap</code> with the   model platform declared in <code>model_config_list</code> and connects the   <code>FileSystemStoragePathSource</code> to it. This way, whenever a new model version is   discovered under the export path, the <code>SavedModelBundleSourceAdapter</code>   adapts it to a <code>Loader&lt;SavedModelBundle&gt;</code>.</li> <li>Instantiates a specific implementation of <code>Manager</code> called   <code>AspiredVersionsManager</code> that manages all such <code>Loader</code> instances created by   the <code>SavedModelBundleSourceAdapter</code>. <code>ServerCore</code> exports the <code>Manager</code>   interface by delegating the calls to <code>AspiredVersionsManager</code>.</li> </ul> <p>Whenever a new version is available, this <code>AspiredVersionsManager</code> loads the new version, and under its default behavior unloads the old one. If you want to start customizing, you are encouraged to understand the components that it creates internally, and how to configure them.</p> <p>It is worth mentioning that TensorFlow Serving is designed from scratch to be very flexible and extensible. You can build various plugins to customize system behavior, while taking advantage of generic core components like <code>ServerCore</code> and <code>AspiredVersionsManager</code>. For example, you could build a data source plugin that monitors cloud storage instead of local storage, or you could build a version policy plugin that does version transition in a different way -- in fact, you could even build a custom model plugin that serves non-TensorFlow models. These topics are out of scope for this tutorial. However, you can refer to the custom source and custom servable tutorials for more information.</p>"},{"location":"guide/serving_advanced/#batching","title":"Batching","text":"<p>Another typical server feature we want in a production environment is batching. Modern hardware accelerators (GPUs, etc.) used to do machine learning inference usually achieve best computation efficiency when inference requests are run in large batches.</p> <p>Batching can be turned on by providing proper <code>SessionBundleConfig</code> when creating the <code>SavedModelBundleSourceAdapter</code>. In this case we set the <code>BatchingParameters</code> with pretty much default values. Batching can be fine-tuned by setting custom timeout, batch_size, etc. values. For details, please refer to <code>BatchingParameters</code>.</p> <pre><code>SessionBundleConfig session_bundle_config;\n// Batching config\nif (enable_batching) {\n  BatchingParameters* batching_parameters =\n      session_bundle_config.mutable_batching_parameters();\n  batching_parameters-&gt;mutable_thread_pool_name()-&gt;set_value(\n      \"model_server_batch_threads\");\n}\n*saved_model_bundle_source_adapter_config.mutable_legacy_config() =\n    session_bundle_config;\n</code></pre> <p>Upon reaching full batch, inference requests are merged internally into a single large request (tensor), and <code>tensorflow::Session::Run()</code> is invoked (which is where the actual efficiency gain on GPUs comes from).</p>"},{"location":"guide/serving_advanced/#serve-with-manager","title":"Serve with Manager","text":"<p>As mentioned above, TensorFlow Serving <code>Manager</code> is designed to be a generic component that can handle loading, serving, unloading and version transition of models generated by arbitrary machine learning systems. Its APIs are built around the following key concepts:</p> <ul> <li> <p>Servable:   Servable is any opaque object that can be used to serve client requests. The   size and granularity of a servable is flexible, such that a single servable   might include anything from a single shard of a lookup table to a single   machine-learned model to a tuple of models. A servable can be of any type and   interface.</p> </li> <li> <p>Servable Version:   Servables are versioned and TensorFlow Serving <code>Manager</code> can manage one or   more versions of a servable. Versioning allows for more than one version of a   servable to be loaded concurrently, supporting gradual rollout and   experimentation.</p> </li> <li> <p>Servable Stream:   A servable stream is the sequence of versions of a servable, with increasing   version numbers.</p> </li> <li> <p>Model:   A machine-learned model is represented by one or more servables. Examples of   servables are:</p> <ul> <li>TensorFlow session or wrappers around them, such as <code>SavedModelBundle</code>.</li> <li>Other kinds of machine-learned models.</li> <li>Vocabulary lookup tables.</li> <li>Embedding lookup tables.</li> </ul> <p>A composite model could be represented as multiple independent servables, or as a single composite servable. A servable may also correspond to a fraction of a Model, for example with a large lookup table sharded across many <code>Manager</code> instances.</p> </li> </ul> <p>To put all these into the context of this tutorial:</p> <ul> <li> <p>TensorFlow models are represented by one kind of servable --     <code>SavedModelBundle</code>. <code>SavedModelBundle</code> internally consists of a     <code>tensorflow:Session</code> paired with some metadata about what graph is loaded     into the session and how to run it for inference.</p> </li> <li> <p>There is a file-system directory containing a stream of TensorFlow exports,     each in its own subdirectory whose name is a version number. The outer     directory can be thought of as the serialized representation of the servable     stream for the TensorFlow model being served. Each export corresponds to a     servables that can be loaded.</p> </li> <li> <p><code>AspiredVersionsManager</code> monitors the export stream, and manages lifecycle     of all <code>SavedModelBundle</code> servables dynamically.</p> </li> </ul> <p><code>TensorflowPredictImpl::Predict</code> then just:</p> <ul> <li>Requests <code>SavedModelBundle</code> from the manager (through ServerCore).</li> <li>Uses the <code>generic signatures</code> to map logical tensor names in   <code>PredictRequest</code> to real tensor names and bind values to tensors.</li> <li>Runs inference.</li> </ul>"},{"location":"guide/serving_advanced/#test-and-run-the-server","title":"Test and run the server","text":"<p>Copy the first version of the export to the monitored folder:</p> <pre><code>mkdir /tmp/monitored\ncp -r /tmp/mnist/1 /tmp/monitored\n</code></pre> <p>Then start the server:</p> <pre><code>docker run -p 8500:8500 \\\n  --mount type=bind,source=/tmp/monitored,target=/models/mnist \\\n  -t --entrypoint=tensorflow_model_server tensorflow/serving --enable_batching \\\n  --port=8500 --model_name=mnist --model_base_path=/models/mnist &amp;\n</code></pre> <p>The server will emit log messages every one second that say \"Aspiring version for servable ...\", which means it has found the export, and is tracking its continued existence.</p> <p>Let's run the client with <code>--concurrency=10</code>. This will send concurrent requests to the server and thus trigger your batching logic.</p> <pre><code>tools/run_in_docker.sh python tensorflow_serving/example/mnist_client.py \\\n  --num_tests=1000 --server=127.0.0.1:8500 --concurrency=10\n</code></pre> <p>Which results in output that looks like:</p> <pre><code>...\nInference error rate: 13.1%\n</code></pre> <p>Then we copy the second version of the export to the monitored folder and re-run the test:</p> <pre><code>cp -r /tmp/mnist/2 /tmp/monitored\ntools/run_in_docker.sh python tensorflow_serving/example/mnist_client.py \\\n  --num_tests=1000 --server=127.0.0.1:8500 --concurrency=10\n</code></pre> <p>Which results in output that looks like:</p> <pre><code>...\nInference error rate: 9.5%\n</code></pre> <p>This confirms that your server automatically discovers the new version and uses it for serving!</p>"},{"location":"guide/serving_basic/","title":"Serving a TensorFlow Model","text":"<p>This tutorial shows you how to use TensorFlow Serving components to export a trained TensorFlow model and use the standard tensorflow_model_server to serve it. If you are already familiar with TensorFlow Serving, and you want to know more about how the server internals work, see the TensorFlow Serving advanced tutorial.</p> <p>This tutorial uses a simple Softmax Regression model that classifies handwritten digits. It is very similar to the one introduced in the TensorFlow tutorial on image classification using the Fashion MNIST dataset.</p> <p>The code for this tutorial consists of two parts:</p> <ul> <li> <p>A Python file,     mnist_saved_model.py,     that trains and exports the model.</p> </li> <li> <p>A ModelServer binary which can be either installed using Apt, or compiled     from a C++ file     (main.cc).     The TensorFlow Serving ModelServer discovers new exported models and runs a     gRPC service for serving them.</p> </li> </ul> <p>Before getting started, first install Docker.</p>"},{"location":"guide/serving_basic/#train-and-export-tensorflow-model","title":"Train and export TensorFlow model","text":"<p>For the training phase, the TensorFlow graph is launched in TensorFlow session <code>sess</code>, with the input tensor (image) as <code>x</code> and output tensor (Softmax score) as <code>y</code>.</p> <p>Then we use TensorFlow's SavedModelBuilder module to export the model. <code>SavedModelBuilder</code> saves a \"snapshot\" of the trained model to reliable storage so that it can be loaded later for inference.</p> <p>For details on the SavedModel format, please see the documentation at SavedModel README.md.</p> <p>From mnist_saved_model.py, the following is a short code snippet to illustrate the general process of saving a model to disk.</p> <pre><code>export_path_base = sys.argv[-1]\nexport_path = os.path.join(\n    tf.compat.as_bytes(export_path_base),\n    tf.compat.as_bytes(str(FLAGS.model_version)))\nprint('Exporting trained model to', export_path)\nbuilder = tf.saved_model.builder.SavedModelBuilder(export_path)\nbuilder.add_meta_graph_and_variables(\n    sess, [tf.compat.v1.saved_model.tag_constants.SERVING],\n    signature_def_map={\n        'predict_images':\n            prediction_signature,\n        tf.compat.v1.saved_model.signature_constants\n            .DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n            classification_signature,\n    },\n    main_op=tf.compat.v1.tables_initializer(),\n    strip_default_attrs=True)\nbuilder.save()\n</code></pre> <p><code>SavedModelBuilder.__init__</code> takes the following argument:</p> <ul> <li><code>export_path</code> is the path of the export directory.</li> </ul> <p><code>SavedModelBuilder</code> will create the directory if it does not exist. In the example, we concatenate the command line argument and <code>FLAGS.model_version</code> to obtain the export directory. <code>FLAGS.model_version</code> specifies the version of the model. You should specify a larger integer value when exporting a newer version of the same model. Each version will be exported to a different sub-directory under the given path.</p> <p>You can add meta graph and variables to the builder using <code>SavedModelBuilder.add_meta_graph_and_variables()</code> with the following arguments:</p> <ul> <li> <p><code>sess</code> is the TensorFlow session that holds the trained model you are     exporting.</p> </li> <li> <p><code>tags</code> is the set of tags with which to save the meta graph. In this case,     since we intend to use the graph in serving, we use the <code>serve</code> tag from     predefined SavedModel tag constants. For more details, see     tag_constants.py     and     related TensorFlow API documentation.</p> </li> <li> <p><code>signature_def_map</code> specifies the map of user-supplied key for a     signature to a tensorflow::SignatureDef to add to the meta graph.     Signature specifies what type of model is being exported, and the     input/output tensors to bind to when running inference.</p> <p>The special signature key <code>serving_default</code> specifies the default serving signature. The default serving signature def key, along with other constants related to signatures, are defined as part of SavedModel signature constants. For more details, see signature_constants.py and related TensorFlow API documentation.</p> <p>Further, to help build signature defs easily, the SavedModel API provides signature def utils.. Specifically, in the original mnist_saved_model.py file, we use <code>signature_def_utils.build_signature_def()</code> to build <code>predict_signature</code> and <code>classification_signature</code>.</p> <p>As an example for how <code>predict_signature</code> is defined, the util takes the following arguments:</p> <ul> <li> <p><code>inputs={'images': tensor_info_x}</code> specifies the input tensor info.</p> </li> <li> <p><code>outputs={'scores': tensor_info_y}</code> specifies the scores tensor info.</p> </li> <li> <p><code>method_name</code> is the method used for the inference. For Prediction     requests, it should be set to <code>tensorflow/serving/predict</code>. For other     method names, see     signature_constants.py     and     related TensorFlow API documentation.</p> </li> </ul> </li> </ul> <p>Note that <code>tensor_info_x</code> and <code>tensor_info_y</code> have the structure of <code>tensorflow::TensorInfo</code> protocol buffer defined here. To easily build tensor infos, the TensorFlow SavedModel API also provides utils.py, with related TensorFlow API documentation.</p> <p>Also, note that <code>images</code> and <code>scores</code> are tensor alias names. They can be whatever unique strings you want, and they will become the logical names of tensor <code>x</code> and <code>y</code> that you refer to for tensor binding when sending prediction requests later.</p> <p>For instance, if <code>x</code> refers to the tensor with name 'long_tensor_name_foo' and <code>y</code> refers to the tensor with name 'generated_tensor_name_bar', <code>builder</code> will store tensor logical name to real name mapping ('images' -&gt; 'long_tensor_name_foo') and ('scores' -&gt; 'generated_tensor_name_bar').  This allows the user to refer to these tensors with their logical names when running inference.</p> <p>Note</p> <p>In addition to the description above, documentation related to signature def structure and how to set up them up can be found here.</p> <p>Let's run it!</p> <p>First, if you haven't done so yet, clone this repository to your local machine:</p> <pre><code>git clone https://github.com/tensorflow/serving.git\ncd serving\n</code></pre> <p>Clear the export directory if it already exists:</p> <pre><code>rm -rf /tmp/mnist\n</code></pre> <p>Now let's train the model:</p> <pre><code>tools/run_in_docker.sh python tensorflow_serving/example/mnist_saved_model.py \\\n  /tmp/mnist\n</code></pre> <p>This should result in output that looks like:</p> <pre><code>Training model...\n\n...\n\nDone training!\nExporting trained model to models/mnist\nDone exporting!\n</code></pre> <p>Now let's take a look at the export directory.</p> <pre><code>$ ls /tmp/mnist\n1\n</code></pre> <p>As mentioned above, a sub-directory will be created for exporting each version of the model. <code>FLAGS.model_version</code> has the default value of 1, therefore the corresponding sub-directory <code>1</code> is created.</p> <pre><code>$ ls /tmp/mnist/1\nsaved_model.pb variables\n</code></pre> <p>Each version sub-directory contains the following files:</p> <ul> <li> <p><code>saved_model.pb</code> is the serialized tensorflow::SavedModel. It includes   one or more graph definitions of the model, as well as metadata of the   model such as signatures.</p> </li> <li> <p><code>variables</code> are files that hold the serialized variables of the graphs.</p> </li> </ul> <p>With that, your TensorFlow model is exported and ready to be loaded!</p>"},{"location":"guide/serving_basic/#load-exported-model-with-standard-tensorflow-modelserver","title":"Load exported model with standard TensorFlow ModelServer","text":"<p>Use a Docker serving image to easily load the model for serving:</p> <pre><code>docker run -p 8500:8500 \\\n--mount type=bind,source=/tmp/mnist,target=/models/mnist \\\n-e MODEL_NAME=mnist -t tensorflow/serving &amp;\n</code></pre>"},{"location":"guide/serving_basic/#test-the-server","title":"Test the server","text":"<p>We can use the provided mnist_client utility to test the server. The client downloads MNIST test data, sends them as requests to the server, and calculates the inference error rate.</p> <pre><code>tools/run_in_docker.sh python tensorflow_serving/example/mnist_client.py \\\n  --num_tests=1000 --server=127.0.0.1:8500\n</code></pre> <p>This should output something like</p> <pre><code>    ...\n    Inference error rate: 11.13%\n</code></pre> <p>We expect around 90% accuracy for the trained Softmax model and we get 11% inference error rate for the first 1000 test images. This confirms that the server loads and runs the trained model successfully!</p>"},{"location":"guide/serving_config/","title":"Tensorflow Serving Configuration","text":"<p>In this guide, we will go over the numerous configuration points for Tensorflow Serving.</p>"},{"location":"guide/serving_config/#overview","title":"Overview","text":"<p>While most configurations relate to the Model Server, there are many ways to specify the behavior of Tensorflow Serving:</p> <ul> <li>Model Server Configuration: Specify model     names, paths, version policy &amp; labels, logging configuration and more</li> <li>Monitoring Configuration: Enable and configure     Prometheus monitoring</li> <li>Batching Configuration: Enable batching and     configure its parameters</li> <li>Misc. Flags: A number of misc. flags that can be     provided to fine-tune the behavior of a Tensorflow Serving deployment</li> </ul>"},{"location":"guide/serving_config/#model-server-configuration","title":"Model Server Configuration","text":"<p>The easiest way to serve a model is to provide the <code>--model_name</code> and <code>--model_base_path</code> flags (or setting the <code>MODEL_NAME</code> environment variable if using Docker). However, if you would like to serve multiple models, or configure options like polling frequency for new versions, you may do so by writing a Model Server config file.</p> <p>You may provide this configuration file using the <code>--model_config_file</code> flag and instruct Tensorflow Serving to periodically poll for updated versions of this configuration file at the specifed path by setting the <code>--model_config_file_poll_wait_seconds</code> flag.</p> <p>Example using Docker:</p> <pre><code>docker run -t --rm -p 8501:8501 \\\n    -v \"$(pwd)/models/:/models/\" tensorflow/serving \\\n    --model_config_file=/models/models.config \\\n    --model_config_file_poll_wait_seconds=60\n</code></pre>"},{"location":"guide/serving_config/#reloading-model-server-configuration","title":"Reloading Model Server Configuration","text":"<p>There are two ways to reload the Model Server configuration:</p> <ul> <li> <p>By setting the <code>--model_config_file_poll_wait_seconds</code> flag to instruct the     server to periodically check for a new config file at <code>--model_config_file</code>     filepath.</p> </li> <li> <p>By issuing     HandleReloadConfigRequest     RPC calls to the server and supplying a new Model Server config     programmatically.</p> </li> </ul> <p>Please note that each time the server loads the new config file, it will act to realize the content of the new specified config and only the new specified config. This means if model A was present in the first config file, which is replaced with a file that contains only model B, the server will load model B and unload model A.</p>"},{"location":"guide/serving_config/#model-server-config-details","title":"Model Server Config Details","text":"<p>The Model Server configuration file provided must be a ModelServerConfig protocol buffer.</p> <p>For all but the most advanced use-cases, you'll want to use the ModelConfigList option, which is a list of ModelConfig protocol buffers. Here's a basic example, before we dive into advanced options below.</p> <pre><code>model_config_list {\n  config {\n    name: 'my_first_model'\n    base_path: '/tmp/my_first_model/'\n    model_platform: 'tensorflow'\n  }\n  config {\n    name: 'my_second_model'\n    base_path: '/tmp/my_second_model/'\n    model_platform: 'tensorflow'\n  }\n}\n</code></pre>"},{"location":"guide/serving_config/#configuring-one-model","title":"Configuring One Model","text":"<p>Each ModelConfig specifies one model to be served, including its name and the path where the Model Server should look for versions of the model to serve, as seen in the above example. By default the server will serve the version with the largest version number. This default can be overridden by changing the model_version_policy field.</p>"},{"location":"guide/serving_config/#serving-a-specific-version-of-a-model","title":"Serving a Specific Version of a Model","text":"<p>To serve a specific version of the model, rather than always transitioning to the one with the largest version number, set model_version_policy to \"specific\" and provide the version number you would like to serve. For example, to pin version 42 as the one to serve:</p> <pre><code>model_version_policy {\n  specific {\n    versions: 42\n  }\n}\n</code></pre> <p>This option is useful for rolling back to a know good version, in the event a problem is discovered with the latest version(s).</p>"},{"location":"guide/serving_config/#serving-multiple-versions-of-a-model","title":"Serving Multiple Versions of a Model","text":"<p>To serve multiple versions of the model simultaneously, e.g. to enable canarying a tentative new version with a slice of traffic, set model_version_policy to \"specific\" and provide multiple version numbers. For example, to serve versions 42 and 43:</p> <pre><code>model_version_policy {\n  specific {\n    versions: 42\n    versions: 43\n  }\n}\n</code></pre>"},{"location":"guide/serving_config/#assigning-string-labels-to-model-versions-to-simplify-canary-and-rollback","title":"Assigning String Labels to Model Versions, To Simplify Canary and Rollback","text":"<p>Sometimes it's helpful to add a level of indirection to model versions. Instead of letting all of your clients know that they should be querying version 42, you can assign an alias such as \"stable\" to whichever version is currently the one clients should query. If you want to redirect a slice of traffic to a tentative canary model version, you can use a second alias \"canary\".</p> <p>You can configure these model version aliases, or labels, like so:</p> <pre><code>model_version_policy {\n  specific {\n    versions: 42\n    versions: 43\n  }\n}\nversion_labels {\n  key: 'stable'\n  value: 42\n}\nversion_labels {\n  key: 'canary'\n  value: 43\n}\n</code></pre> <p>In the above example, you are serving versions 42 and 43, and associating the label \"stable\" with version 42 and the label \"canary\" with version 43. You can have your clients direct queries to one of \"stable\" or \"canary\" (perhaps based on hashing the user id) using the version_label field of the ModelSpec protocol buffer, and move forward the label on the server without notifying the clients. Once you are done canarying version 43 and are ready to promote it to stable, you can update the config to:</p> <pre><code>model_version_policy {\n  specific {\n    versions: 42\n    versions: 43\n  }\n}\nversion_labels {\n  key: 'stable'\n  value: 43\n}\nversion_labels {\n  key: 'canary'\n  value: 43\n}\n</code></pre> <p>If you subsequently need to perform a rollback, you can revert to the old config that has version 42 as \"stable\". Otherwise, you can march forward by unloading version 42 and loading the new version 44 when it is ready, and then advancing the canary label to 44, and so on.</p> <p>Please note that labels can only be assigned to model versions that are already loaded and available for serving. Once a model version is available, one may reload the model config on the fly to assign a label to it. This can be achieved using a HandleReloadConfigRequest RPC or if the server is set up to periodically poll the filesystem for the config file, as described above.</p> <p>If you would like to assign a label to a version that is not yet loaded (for ex. by supplying both the model version and the label at startup time) then you must set the <code>--allow_version_labels_for_unavailable_models</code> flag to true, which allows new labels to be assigned to model versions that are not loaded yet.</p> <p>Please note that this applies only to new version labels (i.e. ones not assigned to a version currently). This is to ensure that during version swaps, the server does not prematurely assign the label to the new version, thereby dropping all requests destined for that label while the new version is loading.</p> <p>In order to comply with this safety check, if re-assigning an already in-use version label, you must assign it only to already-loaded versions. For example, if you would like to move a label from pointing to version N to version N+1, you may first submit a config containing version N and N+1, and then submit a config that contains version N+1, the label pointing to N+1 and no version N.</p>"},{"location":"guide/serving_config/#rest-usage","title":"REST Usage","text":"<p>If you're using the REST API surface to make inference requests, instead of using</p> <p><code>/v1/models/&lt;model name&gt;/versions/&lt;version number&gt;</code></p> <p>simply request a version using a label by structuring your request path like so</p> <p><code>/v1/models/&lt;model name&gt;/labels/&lt;version label&gt;</code>.</p> <p>Note that version label is restricted to a sequence of Word characters, composed of alphanumeral characters and underscores (i.e. <code>[a-zA-Z0-9_]+</code>).</p>"},{"location":"guide/serving_config/#monitoring-configuration","title":"Monitoring Configuration","text":"<p>You may provide a monitoring configuration to the server by using the <code>--monitoring_config_file</code> flag to specify a file containing a MonitoringConfig protocol buffer. Here's an example:</p> <pre><code>prometheus_config {\n  enable: true,\n  path: \"/monitoring/prometheus/metrics\"\n}\n</code></pre> <p>To read metrics from the above monitoring URL, you first need to enable the HTTP server by setting the <code>--rest_api_port</code> flag. You can then configure your Prometheus Server to pull metrics from Model Server by passing it the values of <code>--rest_api_port</code> and <code>path</code>.</p> <p>Tensorflow Serving collects all metrics that are captured by Serving as well as core Tensorflow.</p>"},{"location":"guide/serving_config/#batching-configuration","title":"Batching Configuration","text":"<p>Model Server has the ability to batch requests in a variety of settings in order to realize better throughput. The scheduling for this batching is done globally for all models and model versions on the server to ensure the best possible utilization of the underlying resources no matter how many models or model versions are currently being served by the server (more details). You may enable this behavior by setting the <code>--enable_batching</code> flag and control it by passing a config to the <code>--batching_parameters_file</code> flag.</p> <p>Example batching parameters file:</p> <pre><code>max_batch_size { value: 128 }\nbatch_timeout_micros { value: 0 }\nmax_enqueued_batches { value: 1000000 }\nnum_batch_threads { value: 8 }\n</code></pre> <p>Please refer to the batching guide for an in-depth discussion and refer to the section on parameters to understand how to set the parameters.</p>"},{"location":"guide/serving_config/#miscellaneous-flags","title":"Miscellaneous Flags","text":"<p>In addition to the flags covered so far in the guide, here we list a few other notable ones. For a complete list, please refer to the source code.</p> <ul> <li><code>--port</code>: Port to listen on for gRPC API</li> <li><code>--rest_api_port</code>: Port to listen on for HTTP/REST API</li> <li><code>--rest_api_timeout_in_ms</code>: Timeout for HTTP/REST API calls</li> <li><code>--file_system_poll_wait_seconds</code>: The period with which the server polls     the filesystem for new model versions at each model's respective     model_base_path</li> <li><code>--enable_model_warmup</code>: Enables model warmup using     user-provided PredictionLogs in assets.extra/ directory</li> <li><code>--mixed_precision=bfloat16</code>: Enables BF16 Automatic Mixed Precision</li> </ul>"},{"location":"guide/serving_kubernetes/","title":"Use TensorFlow Serving with Kubernetes","text":"<p>This tutorial shows how to use TensorFlow Serving components running in Docker containers to serve the TensorFlow ResNet model and how to deploy the serving cluster with Kubernetes.</p> <p>To learn more about TensorFlow Serving, we recommend TensorFlow Serving basic tutorial and TensorFlow Serving advanced tutorial.</p> <p>To learn more about TensorFlow ResNet model, we recommend reading ResNet in TensorFlow.</p> <ul> <li>Part 1 gets your environment setup</li> <li>Part 2 shows how to run the local Docker     serving image</li> <li>Part 3 shows how to deploy in Kubernetes.</li> </ul>"},{"location":"guide/serving_kubernetes/#part-1-setup","title":"Part 1: Setup","text":"<p>Before getting started, first install Docker.</p>"},{"location":"guide/serving_kubernetes/#download-the-resnet-savedmodel","title":"Download the ResNet SavedModel","text":"<p>Let's clear our local models directory in case we already have one:</p> <pre><code>rm -rf /tmp/resnet\n</code></pre> <p>Deep residual networks, or ResNets for short, provided the breakthrough idea of identity mappings in order to enable training of very deep convolutional neural networks. For our example, we will download a TensorFlow SavedModel of ResNet for the ImageNet dataset.</p> <pre><code># Download Resnet model from TF Hub\nwget https://tfhub.dev/tensorflow/resnet_50/classification/1?tf-hub-format=compressed -o resnet.tar.gz\n\n# Extract SavedModel into a versioned subfolder \u2018123\u2019\nmkdir -p /tmp/resnet/123\ntar xvfz resnet.tar.gz -C /tmp/resnet/123/\n</code></pre> <p>We can verify we have the SavedModel:</p> <pre><code>$ ls /tmp/resnet/*\nsaved_model.pb  variables\n</code></pre>"},{"location":"guide/serving_kubernetes/#part-2-running-in-docker","title":"Part 2: Running in Docker","text":""},{"location":"guide/serving_kubernetes/#commit-image-for-deployment","title":"Commit image for deployment","text":"<p>Now we want to take a serving image and commit all changes to a new image <code>$USER/resnet_serving</code> for Kubernetes deployment.</p> <p>First we run a serving image as a daemon:</p> <pre><code>docker run -d --name serving_base tensorflow/serving\n</code></pre> <p>Next, we copy the ResNet model data to the container's model folder:</p> <pre><code>docker cp /tmp/resnet serving_base:/models/resnet\n</code></pre> <p>Finally we commit the container to serving the ResNet model:</p> <pre><code>docker commit --change \"ENV MODEL_NAME resnet\" serving_base \\\n  $USER/resnet_serving\n</code></pre> <p>Now let's stop the serving base container</p> <pre><code>docker kill serving_base\ndocker rm serving_base\n</code></pre>"},{"location":"guide/serving_kubernetes/#start-the-server","title":"Start the server","text":"<p>Now let's start the container with the ResNet model so it's ready for serving, exposing the gRPC port 8500:</p> <pre><code>docker run -p 8500:8500 -t $USER/resnet_serving &amp;\n</code></pre>"},{"location":"guide/serving_kubernetes/#query-the-server","title":"Query the server","text":"<p>For the client, we will need to clone the TensorFlow Serving GitHub repo:</p> <pre><code>git clone https://github.com/tensorflow/serving\ncd serving\n</code></pre> <p>Query the server with resnet_client_grpc.py. The client downloads an image and sends it over gRPC for classification into ImageNet categories.</p> <pre><code>tools/run_in_docker.sh python tensorflow_serving/example/resnet_client_grpc.py\n</code></pre> <p>This should result in output like:</p> <pre><code>outputs {\n  key: \"classes\"\n  value {\n    dtype: DT_INT64\n    tensor_shape {\n      dim {\n        size: 1\n      }\n    }\n    int64_val: 286\n  }\n}\noutputs {\n  key: \"probabilities\"\n  value {\n    dtype: DT_FLOAT\n    tensor_shape {\n      dim {\n        size: 1\n      }\n      dim {\n        size: 1001\n      }\n    }\n    float_val: 2.41628322328e-06\n    float_val: 1.90121829746e-06\n    float_val: 2.72477100225e-05\n    float_val: 4.42638565801e-07\n    float_val: 8.98362372936e-07\n    float_val: 6.84421956976e-06\n    float_val: 1.66555237229e-05\n...\n    float_val: 1.59407863976e-06\n    float_val: 1.2315689446e-06\n    float_val: 1.17812135159e-06\n    float_val: 1.46365800902e-05\n    float_val: 5.81210713335e-07\n    float_val: 6.59980651108e-05\n    float_val: 0.00129527016543\n  }\n}\nmodel_spec {\n  name: \"resnet\"\n  version {\n    value: 123\n  }\n  signature_name: \"serving_default\"\n}\n</code></pre> <p>It works! The server successfully classifies a cat image!</p>"},{"location":"guide/serving_kubernetes/#part-3-deploy-in-kubernetes","title":"Part 3: Deploy in Kubernetes","text":"<p>In this section we use the container image built in Part 0 to deploy a serving cluster with Kubernetes in the Google Cloud Platform.</p>"},{"location":"guide/serving_kubernetes/#gcloud-project-login","title":"GCloud project login","text":"<p>Here we assume you have created and logged in a gcloud project named <code>tensorflow-serving</code>.</p> <pre><code>gcloud auth login --project tensorflow-serving\n</code></pre>"},{"location":"guide/serving_kubernetes/#create-a-container-cluster","title":"Create a container cluster","text":"<p>First we create a Google Kubernetes Engine cluster for service deployment.</p> <pre><code>$ gcloud container clusters create resnet-serving-cluster --num-nodes 5\n</code></pre> <p>Which should output something like:</p> <pre><code>Creating cluster resnet-serving-cluster...done.\nCreated [https://container.googleapis.com/v1/projects/tensorflow-serving/zones/us-central1-f/clusters/resnet-serving-cluster].\nkubeconfig entry generated for resnet-serving-cluster.\nNAME                       ZONE           MASTER_VERSION  MASTER_IP        MACHINE_TYPE   NODE_VERSION  NUM_NODES  STATUS\nresnet-serving-cluster  us-central1-f  1.1.8           104.197.163.119  n1-standard-1  1.1.8         5          RUNNING\n</code></pre> <p>Set the default cluster for gcloud container command and pass cluster credentials to kubectl.</p> <pre><code>gcloud config set container/cluster resnet-serving-cluster\ngcloud container clusters get-credentials resnet-serving-cluster\n</code></pre> <p>which should result in:</p> <pre><code>Fetching cluster endpoint and auth data.\nkubeconfig entry generated for resnet-serving-cluster.\n</code></pre>"},{"location":"guide/serving_kubernetes/#upload-the-docker-image","title":"Upload the Docker image","text":"<p>Let's now push our image to the Google Container Registry so that we can run it on Google Cloud Platform.</p> <p>First we tag the <code>$USER/resnet_serving</code> image using the Container Registry format and our project name,</p> <pre><code>docker tag $USER/resnet_serving gcr.io/tensorflow-serving/resnet\n</code></pre> <p>Next, we configure Docker to use gcloud as a credential helper:</p> <pre><code>gcloud auth configure-docker\n</code></pre> <p>Next we push the image to the Registry,</p> <pre><code>docker push gcr.io/tensorflow-serving/resnet\n</code></pre>"},{"location":"guide/serving_kubernetes/#create-kubernetes-deployment-and-service","title":"Create Kubernetes Deployment and Service","text":"<p>The deployment consists of 3 replicas of <code>resnet_inference</code> server controlled by a Kubernetes Deployment. The replicas are exposed externally by a Kubernetes Service along with an External Load Balancer.</p> <p>We create them using the example Kubernetes config resnet_k8s.yaml.</p> <pre><code>kubectl create -f tensorflow_serving/example/resnet_k8s.yaml\n</code></pre> <p>With output:</p> <pre><code>deployment \"resnet-deployment\" created\nservice \"resnet-service\" created\n</code></pre> <p>To view status of the deployment and pods:</p> <pre><code>$ kubectl get deployments\nNAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nresnet-deployment    3         3         3            3           5s\n</code></pre> <pre><code>$ kubectl get pods\nNAME                         READY     STATUS    RESTARTS   AGE\nresnet-deployment-bbcbc   1/1       Running   0          10s\nresnet-deployment-cj6l2   1/1       Running   0          10s\nresnet-deployment-t1uep   1/1       Running   0          10s\n</code></pre> <p>To view status of the service:</p> <pre><code>$ kubectl get services\nNAME                    CLUSTER-IP       EXTERNAL-IP       PORT(S)     AGE\nresnet-service       10.239.240.227   104.155.184.157   8500/TCP    1m\n</code></pre> <p>It can take a while for everything to be up and running.</p> <pre><code>$ kubectl describe service resnet-service\nName:           resnet-service\nNamespace:      default\nLabels:         run=resnet-service\nSelector:       run=resnet-service\nType:           LoadBalancer\nIP:         10.239.240.227\nLoadBalancer Ingress:   104.155.184.157\nPort:           &lt;unset&gt; 8500/TCP\nNodePort:       &lt;unset&gt; 30334/TCP\nEndpoints:      &lt;none&gt;\nSession Affinity:   None\nEvents:\n  FirstSeen LastSeen    Count   From            SubobjectPath   Type        Reason      Message\n  --------- --------    -----   ----            -------------   --------    ------      -------\n  1m        1m      1   {service-controller }           Normal      CreatingLoadBalancer    Creating load balancer\n  1m        1m      1   {service-controller }           Normal      CreatedLoadBalancer Created load balancer\n</code></pre> <p>The service external IP address is listed next to LoadBalancer Ingress.</p>"},{"location":"guide/serving_kubernetes/#query-the-model","title":"Query the model","text":"<p>We can now query the service at its external address from our local host.</p> <pre><code>$ tools/run_in_docker.sh python \\\n  tensorflow_serving/example/resnet_client_grpc.py \\\n  --server=104.155.184.157:8500\noutputs {\n  key: \"classes\"\n  value {\n    dtype: DT_INT64\n    tensor_shape {\n      dim {\n        size: 1\n      }\n    }\n    int64_val: 286\n  }\n}\noutputs {\n  key: \"probabilities\"\n  value {\n    dtype: DT_FLOAT\n    tensor_shape {\n      dim {\n        size: 1\n      }\n      dim {\n        size: 1001\n      }\n    }\n    float_val: 2.41628322328e-06\n    float_val: 1.90121829746e-06\n    float_val: 2.72477100225e-05\n    float_val: 4.42638565801e-07\n    float_val: 8.98362372936e-07\n    float_val: 6.84421956976e-06\n    float_val: 1.66555237229e-05\n...\n    float_val: 1.59407863976e-06\n    float_val: 1.2315689446e-06\n    float_val: 1.17812135159e-06\n    float_val: 1.46365800902e-05\n    float_val: 5.81210713335e-07\n    float_val: 6.59980651108e-05\n    float_val: 0.00129527016543\n  }\n}\nmodel_spec {\n  name: \"resnet\"\n  version {\n    value: 1538687457\n  }\n  signature_name: \"serving_default\"\n}\n</code></pre> <p>You have successfully deployed the ResNet model serving as a service in Kubernetes!</p>"},{"location":"guide/setup/","title":"Installation","text":""},{"location":"guide/setup/#installing-modelserver","title":"Installing ModelServer","text":""},{"location":"guide/setup/#installing-using-docker","title":"Installing using Docker","text":"<p>The easiest and most straight-forward way of using TensorFlow Serving is with Docker images. We highly recommend this route unless you have specific needs that are not addressed by running in a container.</p> <p>Tip</p> <p>This is also the easiest way to get TensorFlow Serving working with GPU support.</p>"},{"location":"guide/setup/#installing-using-apt","title":"Installing using APT","text":""},{"location":"guide/setup/#available-binaries","title":"Available binaries","text":"<p>The TensorFlow Serving ModelServer binary is available in two variants:</p> <p>tensorflow-model-server: Fully optimized server that uses some platform specific compiler optimizations like SSE4 and AVX instructions. This should be the preferred option for most users, but may not work on some older machines.</p> <p>tensorflow-model-server-universal: Compiled with basic optimizations, but doesn't include platform specific instruction sets, so should work on most if not all machines out there. Use this if <code>tensorflow-model-server</code> does not work for you. Note that the binary name is the same for both packages, so if you already installed tensorflow-model-server, you should first uninstall it using</p> <pre><code>apt-get remove tensorflow-model-server\n</code></pre>"},{"location":"guide/setup/#installation_1","title":"Installation","text":"<ol> <li> <p>Add TensorFlow Serving distribution URI as a package source (one time setup)</p> <pre><code>echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list &amp;&amp; \\\ncurl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n</code></pre> </li> <li> <p>Install and update TensorFlow ModelServer</p> <pre><code>apt-get update &amp;&amp; apt-get install tensorflow-model-server\n</code></pre> </li> </ol> <p>Once installed, the binary can be invoked using the command     <code>tensorflow_model_server</code>.</p> <p>You can upgrade to a newer version of tensorflow-model-server with:</p> <pre><code>apt-get upgrade tensorflow-model-server\n</code></pre> <p>Note</p> <p>In the above commands, replace tensorflow-model-server with tensorflow-model-server-universal if your processor does not support AVX instructions.</p>"},{"location":"guide/setup/#building-from-source","title":"Building from source","text":"<p>The recommended approach to building from source is to use Docker. The TensorFlow Serving Docker development images encapsulate all the dependencies you need to build your own version of TensorFlow Serving.</p> <p>For a listing of what these dependencies are, see the TensorFlow Serving Development Dockerfiles [CPU, GPU].</p> <p>Note</p> <p>Currently we only support building binaries that run on Linux.</p>"},{"location":"guide/setup/#installing-docker","title":"Installing Docker","text":"<p>General installation instructions are on the Docker site.</p>"},{"location":"guide/setup/#clone-the-build-script","title":"Clone the build script","text":"<p>After installing Docker, we need to get the source we want to build from. We will use Git to clone the master branch of TensorFlow Serving:</p> <pre><code>git clone https://github.com/tensorflow/serving.git\ncd serving\n</code></pre>"},{"location":"guide/setup/#build","title":"Build","text":"<p>In order to build in a hermetic environment with all dependencies taken care of, we will use the <code>run_in_docker.sh</code> script. This script passes build commands through to a Docker container. By default, the script will build with the latest nightly Docker development image.</p> <p>TensorFlow Serving uses Bazel as its build tool. You can use Bazel commands to build individual targets or the entire source tree.</p> <p>To build the entire tree, execute:</p> <pre><code>tools/run_in_docker.sh bazel build -c opt tensorflow_serving/...\n</code></pre> <p>Binaries are placed in the bazel-bin directory, and can be run using a command like:</p> <pre><code>bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server\n</code></pre> <p>To test your build, execute:</p> <pre><code>tools/run_in_docker.sh bazel test -c opt tensorflow_serving/...\n</code></pre> <p>See the basic tutorial and advanced tutorial for more in-depth examples of running TensorFlow Serving.</p>"},{"location":"guide/setup/#building-specific-versions-of-tensorflow-serving","title":"Building specific versions of TensorFlow Serving","text":"<p>If you want to build from a specific branch (such as a release branch), pass <code>-b &lt;branchname&gt;</code> to the <code>git clone</code> command.</p> <p>We will also want to match the build environment for that branch of code, by passing the <code>run_in_docker.sh</code> script the Docker development image we'd like to use.</p> <p>For example, to build version 1.10 of TensorFlow Serving:</p> <pre><code>$ git clone -b r1.10 https://github.com/tensorflow/serving.git\n...\n$ cd serving\n$ tools/run_in_docker.sh -d tensorflow/serving:1.10-devel \\\n  bazel build tensorflow_serving/...\n...\n</code></pre>"},{"location":"guide/setup/#optimized-build","title":"Optimized build","text":"<p>If you'd like to apply generally recommended optimizations, including utilizing platform-specific instruction sets for your processor, you can add <code>--config=nativeopt</code> to Bazel build commands when building TensorFlow Serving.</p> <p>For example:</p> <pre><code>tools/run_in_docker.sh bazel build --config=nativeopt tensorflow_serving/...\n</code></pre> <p>It's also possible to compile using specific instruction sets (e.g. AVX). Wherever you see <code>bazel build</code> in the documentation, simply add the corresponding flags:</p> Instruction Set Flags AVX <code>--copt=-mavx</code> AVX2 <code>--copt=-mavx2</code> FMA <code>--copt=-mfma</code> SSE 4.1 <code>--copt=-msse4.1</code> SSE 4.2 <code>--copt=-msse4.2</code> All supported by processor <code>--copt=-march=native</code> <p>For example:</p> <pre><code>tools/run_in_docker.sh bazel build --copt=-mavx2 tensorflow_serving/...\n</code></pre> <p>Note</p> <p>These instruction sets are not available on all machines, especially with older processors. Use the default <code>--config=nativeopt</code> to build an optimized version of TensorFlow Serving for your processor if you are in doubt.</p>"},{"location":"guide/setup/#building-with-gpu-support","title":"Building with GPU Support","text":"<p>In order to build a custom version of TensorFlow Serving with GPU support, we recommend either building with the provided Docker images, or following the approach in the GPU Dockerfile.</p>"},{"location":"guide/setup/#tensorflow-serving-python-api-pip-package","title":"TensorFlow Serving Python API PIP package","text":"<p>To run Python client code without the need to build the API, you can install the <code>tensorflow-serving-api</code> PIP package using:</p> <pre><code>pip install tensorflow-serving-api\n</code></pre>"},{"location":"guide/signature_defs/","title":"SignatureDefs in SavedModel for TensorFlow Serving","text":""},{"location":"guide/signature_defs/#objective","title":"Objective","text":"<p>This document provides examples for the intended usage of SignatureDefs in SavedModel that map to TensorFlow Serving's APIs.</p>"},{"location":"guide/signature_defs/#overview","title":"Overview","text":"<p>A SignatureDef defines the signature of a computation supported in a TensorFlow graph. SignatureDefs aim to provide generic support to identify inputs and outputs of a function and can be specified when building a SavedModel.</p>"},{"location":"guide/signature_defs/#background","title":"Background","text":"<p>TF-Exporter and SessionBundle used Signatures which are similar in concept but required users to distinguish between named and default signatures in order for them to be retrieved correctly upon a load. For those who previously used TF-Exporter/SessionBundle, <code>Signatures</code> in TF-Exporter will be replaced by <code>SignatureDefs</code> in SavedModel.</p>"},{"location":"guide/signature_defs/#signaturedef-structure","title":"SignatureDef Structure","text":"<p>A SignatureDef requires specification of:</p> <ul> <li><code>inputs</code> as a map of string to TensorInfo.</li> <li><code>outputs</code> as a map of string to TensorInfo.</li> <li><code>method_name</code> (which corresponds to a supported method name in the loading     tool/system).</li> </ul> <p>Note that TensorInfo itself requires specification of name, dtype and tensor shape. While tensor information is already present in the graph, it is useful to explicitly have the TensorInfo defined as part of the SignatureDef since tools can then perform signature validation, etc. without having to read the graph definition.</p>"},{"location":"guide/signature_defs/#related-constants-and-utils","title":"Related constants and utils","text":"<p>For ease of reuse and sharing across tools and systems, commonly used constants related to SignatureDefs that will be supported in TensorFlow Serving are defined as constants. Specifically:</p> <ul> <li>Signature constants in     Python.</li> <li>Signature constants in     C++.</li> </ul> <p>In addition, SavedModel provides a util to help build a signature-def.</p>"},{"location":"guide/signature_defs/#sample-structures","title":"Sample structures","text":"<p>TensorFlow Serving provides high level APIs for performing inference. To enable these APIs, models must include one or more SignatureDefs that define the exact TensorFlow nodes to use for input and output. See below for examples of the specific SignatureDefs that TensorFlow Serving supports for each API.</p> <p>Note that TensorFlow Serving depends on the keys of each TensorInfo (in the inputs and outputs of the SignatureDef), as well as the method_name of the SignatureDef. The actual contents of the TensorInfo are specific to your graph.</p>"},{"location":"guide/signature_defs/#classification-signaturedef","title":"Classification SignatureDef","text":"<p>Classification SignatureDefs support structured calls to TensorFlow Serving's Classification API. These prescribe that there must be an <code>inputs</code> Tensor, and that there are two optional output Tensors: <code>classes</code> and <code>scores</code>, at least one of which must be present.</p> <pre><code>signature_def: {\n  key  : \"my_classification_signature\"\n  value: {\n    inputs: {\n      key  : \"inputs\"\n      value: {\n        name: \"tf_example:0\"\n        dtype: DT_STRING\n        tensor_shape: ...\n      }\n    }\n    outputs: {\n      key  : \"classes\"\n      value: {\n        name: \"index_to_string:0\"\n        dtype: DT_STRING\n        tensor_shape: ...\n      }\n    }\n    outputs: {\n      key  : \"scores\"\n      value: {\n        name: \"TopKV2:0\"\n        dtype: DT_FLOAT\n        tensor_shape: ...\n      }\n    }\n    method_name: \"tensorflow/serving/classify\"\n  }\n}\n</code></pre>"},{"location":"guide/signature_defs/#predict-signaturedef","title":"Predict SignatureDef","text":"<p>Predict SignatureDefs support calls to TensorFlow Serving's Predict API. These signatures allow you to flexibly support arbitrarily many input and output Tensors. For the example below, the signature <code>my_prediction_signature</code> has a single logical input Tensor <code>images</code> that are mapped to the actual Tensor in your graph <code>x:0</code>.</p> <p>Predict SignatureDefs enable portability across models. This means that you can swap in different SavedModels, possibly with different underlying Tensor names (e.g. instead of <code>x:0</code> perhaps you have a new alternate model with a Tensor <code>z:0</code>), while your clients can stay online continuously querying the old and new versions of this model without client-side changes.</p> <p>Predict SignatureDefs also allow you to add optional additional Tensors to the outputs, that you can explicitly query. Let's say that in addition to the output key below of <code>scores</code>, you also wanted to fetch a pooling layer for debugging or other purposes. In that case, you would simply add an additional Tensor with a key like <code>pool</code> and appropriate value.</p> <pre><code>signature_def: {\n  key  : \"my_prediction_signature\"\n  value: {\n    inputs: {\n      key  : \"images\"\n      value: {\n        name: \"x:0\"\n        dtype: ...\n        tensor_shape: ...\n      }\n    }\n    outputs: {\n      key  : \"scores\"\n      value: {\n        name: \"y:0\"\n        dtype: ...\n        tensor_shape: ...\n      }\n    }\n    method_name: \"tensorflow/serving/predict\"\n  }\n}\n</code></pre>"},{"location":"guide/signature_defs/#regression-signaturedef","title":"Regression SignatureDef","text":"<p>Regression SignatureDefs support structured calls to TensorFlow Serving's Regression API. These prescribe that there must be exactly one <code>inputs</code> Tensor, and one <code>outputs</code> Tensor.</p> <pre><code>signature_def: {\n  key  : \"my_regression_signature\"\n  value: {\n    inputs: {\n      key  : \"inputs\"\n      value: {\n        name: \"x_input_examples_tensor_0\"\n        dtype: ...\n        tensor_shape: ...\n      }\n    }\n    outputs: {\n      key  : \"outputs\"\n      value: {\n        name: \"y_outputs_0\"\n        dtype: DT_FLOAT\n        tensor_shape: ...\n      }\n    }\n    method_name: \"tensorflow/serving/regress\"\n  }\n}\n</code></pre>"},{"location":"tutorials/building_with_docker/","title":"Building with docker","text":""},{"location":"tutorials/building_with_docker/#developing-with-docker","title":"Developing with Docker","text":""},{"location":"tutorials/building_with_docker/#pulling-a-development-image","title":"Pulling a development image","text":"<p>For a development environment where you can build TensorFlow Serving, you can try:</p> <pre><code>docker pull tensorflow/serving:latest-devel\n</code></pre> <p>For a development environment where you can build TensorFlow Serving with GPU support, use:</p> <pre><code>docker pull tensorflow/serving:latest-devel-gpu\n</code></pre> <p>See the Docker Hub tensorflow/serving repo for other versions of images you can pull.</p>"},{"location":"tutorials/building_with_docker/#development-example","title":"Development example","text":"<p>After pulling one of the development Docker images, you can run it while opening the gRPC port (8500):</p> <pre><code>docker run -it -p 8500:8500 --gpus all tensorflow/serving:latest-devel\n</code></pre>"},{"location":"tutorials/building_with_docker/#testing-the-development-environment","title":"Testing the development environment","text":"<p>To test a model, from inside the container try:</p> <pre><code># train the mnist model\npython tensorflow_serving/example/mnist_saved_model.py /tmp/mnist_model\n# serve the model\ntensorflow_model_server --port=8500 --model_name=mnist --model_base_path=/tmp/mnist_model/ &amp;\n# test the client\npython tensorflow_serving/example/mnist_client.py --num_tests=1000 --server=localhost:8500\n</code></pre>"},{"location":"tutorials/building_with_docker/#dockerfiles","title":"Dockerfiles","text":"<p>We currently maintain the following Dockerfiles:</p> <ul> <li> <p><code>Dockerfile</code>,     which is a minimal VM with TensorFlow Serving installed.</p> </li> <li> <p><code>Dockerfile.gpu</code>,     which is a minimal VM with TensorFlow Serving with GPU support to be used     with <code>nvidia-docker</code>.</p> </li> <li> <p><code>Dockerfile.devel</code>,     which is a minimal VM with all of the dependencies needed to build     TensorFlow Serving.</p> </li> <li> <p><code>Dockerfile.devel-gpu</code>,     which is a minimal VM with all of the dependencies needed to build     TensorFlow Serving with GPU support.</p> </li> </ul>"},{"location":"tutorials/building_with_docker/#building-a-container-from-a-dockerfile","title":"Building a container from a Dockerfile","text":"<p>If you'd like to build your own Docker image from a Dockerfile, you can do so by running the Docker build command:</p> <p><code>Dockerfile</code>:</p> <pre><code>docker build --pull -t $USER/tensorflow-serving .\n</code></pre> <p><code>Dockerfile.gpu</code>:</p> <pre><code>docker build --pull -t $USER/tensorflow-serving-gpu -f Dockerfile.gpu .\n</code></pre> <p><code>Dockerfile.devel</code>:</p> <pre><code>docker build --pull -t $USER/tensorflow-serving-devel -f Dockerfile.devel .\n</code></pre> <p><code>Dockerfile.devel-gpu</code>:</p> <pre><code>docker build --pull -t $USER/tensorflow-serving-devel-gpu -f Dockerfile.devel-gpu .\n</code></pre> <p>Tip</p> <p>Before attempting to build an image, check the Docker Hub tensorflow/serving repo to make sure an image that meets your needs doesn't already exist.</p> <p>Building from sources consumes a lot of RAM. If RAM is an issue on your system, you may limit RAM usage by specifying <code>--local_ram_resources=2048</code> while invoking Bazel. See the Bazel docs for more information. You can use this same mechanism to tweak the optmizations you're building TensorFlow Serving with. For example:</p> <pre><code>docker build --pull --build-arg TF_SERVING_BUILD_OPTIONS=\"--copt=-mavx \\\n  --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --local_ram_resources=2048\" -t \\\n  $USER/tensorflow-serving-devel -f Dockerfile.devel .\n</code></pre>"},{"location":"tutorials/building_with_docker/#running-a-container","title":"Running a container","text":"<p>This assumes you have built the <code>Dockerfile.devel</code> container.</p> <p>To run the container opening the gRPC port (8500):</p> <pre><code>docker run -it -p 8500:8500 $USER/tensorflow-serving-devel\n</code></pre> <p>Tip</p> <p>If you're running a GPU image, be sure to run using the NVIDIA runtime <code>--runtime=nvidia</code>.</p> <p>From here, you can follow the instructions for testing a development environment.</p>"},{"location":"tutorials/building_with_docker/#building-an-optimized-serving-binary","title":"Building an optimized serving binary","text":"<p>When running TensorFlow Serving's ModelServer, you may notice a log message that looks like this:</p> <pre><code>I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141]\nYour CPU supports instructions that this TensorFlow binary was not compiled to\nuse: AVX2 FMA\n</code></pre> <p>This indicates that your ModelServer binary isn't fully optimized for the CPU its running on. Depending on the model you are serving, further optimizations may not be necessary. However, building an optimized binary is straight-forward.</p> <p>When building a Docker image from the provided <code>Dockerfile.devel</code> or <code>Dockerfile.devel-gpu</code> files, the ModelServer binary will be built with the flag <code>-march=native</code>. This will cause Bazel to build a ModelServer binary with all of the CPU optimizations the host you're building the Docker image on supports.</p> <p>To create a serving image that's fully optimized for your host, simply:</p> <ol> <li> <p>Clone the TensorFlow Serving project</p> <pre><code>git clone https://github.com/tensorflow/serving\ncd serving\n</code></pre> </li> <li> <p>Build an image with an optimized ModelServer</p> <ul> <li> <p>For CPU:</p> <pre><code>docker build --pull -t $USER/tensorflow-serving-devel \\\n  -f tensorflow_serving/tools/docker/Dockerfile.devel .\n</code></pre> </li> <li> <p>For GPU: `</p> <pre><code>docker build --pull -t $USER/tensorflow-serving-devel-gpu \\\n  -f tensorflow_serving/tools/docker/Dockerfile.devel-gpu .\n</code></pre> </li> </ul> </li> <li> <p>Build a serving image with the development image as a base</p> <ul> <li> <p>For CPU:</p> <pre><code>docker build -t $USER/tensorflow-serving \\\n  --build-arg TF_SERVING_BUILD_IMAGE=$USER/tensorflow-serving-devel \\\n  -f tensorflow_serving/tools/docker/Dockerfile .\n</code></pre> <p>Your new optimized Docker image is now <code>$USER/tensorflow-serving</code>, which you can use just as you would the standard <code>tensorflow/serving:latest</code> image.</p> </li> <li> <p>For GPU:</p> <pre><code>docker build -t $USER/tensorflow-serving-gpu \\\n  --build-arg TF_SERVING_BUILD_IMAGE=$USER/tensorflow-serving-devel-gpu \\\n  -f tensorflow_serving/tools/docker/Dockerfile.gpu .\n</code></pre> <p>Your new optimized Docker image is now <code>$USER/tensorflow-serving-gpu</code>, which you can use just as you would the standard <code>tensorflow/serving:latest-gpu</code> image.</p> </li> </ul> </li> </ol>"},{"location":"tutorials/performance/","title":"Performance Guide","text":"<p>The performance of TensorFlow Serving is highly dependent on the application it runs, the environment in which it is deployed and other software with which it shares access to the underlying hardware resources. As such, tuning its performance is somewhat case-dependent and there are very few universal rules that are guaranteed to yield optimal performance in all settings. With that said, this document aims to capture some general principles and best practices for running TensorFlow Serving.</p> <p>Please use the Profile Inference Requests with TensorBoard guide to understand the underlying behavior of your model's computation on inference requests, and use this guide to iteratively improve its performance.</p> <p>Note</p> <p>If the following quick tips do not solve your problem, please read the longer discussion to develop a deep understanding of what affects TensorFlow Serving's performance.</p>"},{"location":"tutorials/performance/#quick-tips","title":"Quick Tips","text":"<ul> <li>Latency of first request is too high? Enable     model warmup.</li> <li>Interested in higher resource utilization or throughput? Configure     batching</li> </ul>"},{"location":"tutorials/performance/#performance-tuning-objectives-and-parameters","title":"Performance Tuning: Objectives and Parameters","text":"<p>When fine-tuning TensorFlow Serving's performance, there are usually 2 types of objectives you may have and 3 groups of parameters to tweak to improve upon those objectives.</p>"},{"location":"tutorials/performance/#objectives","title":"Objectives","text":"<p>TensorFlow Serving is an online serving system for machine-learned models. As with many other online serving systems, its primary performance objective is to maximize throughput while keeping tail-latency below certain bounds. Depending on the details and maturity of your application, you may care more about average latency than tail-latency, but some notion of latency and throughput are usually the metrics against which you set performance objectives. Note that we do not discuss availability in this guide as that is more a function of the deployment environment.</p>"},{"location":"tutorials/performance/#parameters","title":"Parameters","text":"<p>We can roughly think about 3 groups of parameters whose configuration determines observed performance: 1) the TensorFlow model 2) the inference requests and 3) the server (hardware &amp; binary).</p>"},{"location":"tutorials/performance/#1-the-tensorflow-model","title":"1) The TensorFlow Model","text":"<p>The model defines the computation that TensorFlow Serving will perform upon receiving each incoming request.</p> <p>Underneath the hood, TensorFlow Serving uses the TensorFlow runtime to do the actual inference on your requests. This means the average latency of serving a request with TensorFlow Serving is usually at least that of doing inference directly with TensorFlow. This means if on a given machine, inference on a single example takes 2 seconds, and you have a sub-second latency target, you need to profile inference requests, understand what TensorFlow ops and sub-graphs of your model contribute most to that latency, and re-design your model with inference latency as a design constraint in mind.</p> <p>Please note, while the average latency of performing inference with TensorFlow Serving is usually not lower than using TensorFlow directly, where TensorFlow Serving shines is keeping the tail latency down for many clients querying many different models, all while efficiently utilizing the underlying hardware to maximize throughput.</p>"},{"location":"tutorials/performance/#2-the-inference-requests","title":"2) The Inference Requests","text":""},{"location":"tutorials/performance/#api-surfaces","title":"API Surfaces","text":"<p>TensorFlow Serving has two API surfaces (HTTP and gRPC), both of which implement the PredictionService API (with the exception of the HTTP Server not exposing a <code>MultiInference</code> endpoint). Both API surfaces are highly tuned and add minimal latency but in practice, the gRPC surface is observed to be slightly more performant.</p>"},{"location":"tutorials/performance/#api-methods","title":"API Methods","text":"<p>In general, it is advised to use the Classify and Regress endpoints as they accept tf.Example, which is a higher-level abstraction; however, in rare cases of large (O(Mb)) structured requests, savvy users may find using PredictRequest and directly encoding their Protobuf messages into a TensorProto, and skipping the serialization into and deserialization from tf.Example a source of slight performance gain.</p>"},{"location":"tutorials/performance/#batch-size","title":"Batch Size","text":"<p>There are two primary ways batching can help your performance. You may configure your clients to send batched requests to TensorFlow Serving, or you may send individual requests and configure TensorFlow Serving to wait up to a predetermined period of time, and perform inference on all requests that arrive in that period in one batch. Configuring the latter kind of batching allows you to hit TensorFlow Serving at extremely high QPS, while allowing it to sub-linearly scale the compute resources needed to keep up. This is further discussed in the configuration guide and the batching README.</p>"},{"location":"tutorials/performance/#3-the-server-hardware-binary","title":"3) The Server (Hardware &amp; Binary)","text":"<p>The TensorFlow Serving binary does fairly precise accounting of the hardware upon which it runs. As such, you should avoid running other compute- or memory-intensive applications on the same machine, especially ones with dynamic resource usage.</p> <p>As with many other types of workloads, TensorFlow Serving is more efficient when deployed on fewer, larger (more CPU and RAM) machines (i.e. a <code>Deployment</code> with a lower <code>replicas</code> in Kubernetes terms). This is due to a better potential for multi-tenant deployment to utilize the hardware and lower fixed costs (RPC server, TensorFlow runtime, etc.).</p>"},{"location":"tutorials/performance/#accelerators","title":"Accelerators","text":"<p>If your host has access to an accelerator, ensure you have implemented your model to place dense computations on the accelerator - this should be automatically done if you have used high-level TensorFlow APIs, but if you have built custom graphs, or want to pin specific parts of graphs on specific accelerators, you may need to manually place certain subgraphs on accelerators (i.e. using <code>with tf.device('/device:GPU:0'): ...</code>).</p>"},{"location":"tutorials/performance/#modern-cpus","title":"Modern CPUs","text":"<p>Modern CPUs have continuously extended the x86 instruction set architecture to improve support for SIMD (Single Instruction Multiple Data) and other features critical for dense computations (eg. a multiply and addition in one clock cycle). However, in order to run on slightly older machines, TensorFlow and TensorFlow Serving are built with the modest assumption that the newest of these features are not supported by the host CPU.</p> <p><code>Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</code></p> <p>If you see this log entry (possibly different extensions than the 2 listed) at TensorFlow Serving start-up, it means you can rebuild TensorFlow Serving and target your particular host's platform and enjoy better performance. Building TensorFlow Serving from source is relatively easy using Docker and is documented here.</p>"},{"location":"tutorials/performance/#binary-configuration","title":"Binary Configuration","text":"<p>TensorFlow Serving offers a number of configuration knobs that govern its runtime behavior, mostly set through command-line flags. Some of these (most notably <code>tensorflow_intra_op_parallelism</code> and <code>tensorflow_inter_op_parallelism</code>) are passed down to configure the TensorFlow runtime and are auto-configured, which savvy users may override by doing many experiments and finding the right configuration for their specific workload and environment.</p>"},{"location":"tutorials/performance/#life-of-a-tensorflow-serving-inference-request","title":"Life of a TensorFlow Serving inference request","text":"<p>Let's briefly go through the life of a prototypical example of a TensorFlow Serving inference request to see the journey that a typical request goes through. For our example, we will dive into a Predict Request being received by the 2.0.0 TensorFlow Serving gRPC API surface.</p> <p>Let's first look at a component-level sequence diagram, and then jump into the code that implements this series of interactions.</p>"},{"location":"tutorials/performance/#sequence-diagram","title":"Sequence Diagram","text":"<p>Note that Client is a component owned by the user, Prediction Service, Servables and Server Core are owned by TensorFlow Serving and TensorFlow Runtime is owned by Core TensorFlow.</p>"},{"location":"tutorials/performance/#sequence-details","title":"Sequence Details","text":"<ol> <li><code>PredictionServiceImpl::Predict</code>     receives the <code>PredictRequest</code></li> <li>We invoke the     <code>TensorflowPredictor::Predict</code>,     propagating the request deadline from the gRPC request (if one was set).</li> <li>Inside <code>TensorflowPredictor::Predict</code>, we     lookup the Servable (model)     the request is looking to perform inference on, from which we retrieve     information about the SavedModel and more importantly, a handle to the     <code>Session</code> object in which the model graph is (possibly partially) loaded.     This Servable object was created and committed in memory when the model was     loaded by TensorFlow Serving. We then invoke     internal::RunPredict     to carry out the prediction.</li> <li>In <code>internal::RunPredict</code>, after validating and preprocessing the request,     we use the <code>Session</code> object to perform the inference using a blocking call     to     Session::Run,     at which point, we enter core TensorFlow's codebase. After the     <code>Session::Run</code> returns and our <code>outputs</code> tensors have been populated, we     convert     the outputs to a <code>PredictionResponse</code> and return the result up the call     stack.</li> </ol>"},{"location":"tutorials/saved_model_warmup/","title":"SavedModel Warmup","text":""},{"location":"tutorials/saved_model_warmup/#introduction","title":"Introduction","text":"<p>The TensorFlow runtime has components that are lazily initialized, which can cause high latency for the first request/s sent to a model after it is loaded. This latency can be several orders of magnitude higher than that of a single inference request.</p> <p>To reduce the impact of lazy initialization on request latency, it's possible to trigger the initialization of the sub-systems and components at model load time by providing a sample set of inference requests along with the SavedModel. This process is known as \"warming up\" the model.</p>"},{"location":"tutorials/saved_model_warmup/#usage","title":"Usage","text":"<p>SavedModel Warmup is supported for Regress, Classify, MultiInference and Predict. To trigger warmup of the model at load time, attach a warmup data file under the assets.extra subfolder of the SavedModel directory.</p> <p>Requirements for model warmup to work correctly:</p> <ul> <li>Warmup file name: 'tf_serving_warmup_requests'</li> <li>File location: assets.extra/</li> <li>File format:     TFRecord     with each record as a     PredictionLog.</li> <li>Number of warmup records &lt;= 1000.</li> <li>The warmup data must be representative of the inference requests used at     serving.</li> </ul>"},{"location":"tutorials/saved_model_warmup/#warm-up-data-generation","title":"Warm-up data generation","text":"<p>Warmup data can be added in two ways:</p> <ul> <li>By directly populating the warmup requests into your exported Saved Model.     This could be done by creating a script reading a list of sample     inference requests, converting each request into     PredictionLog     (if it's originally in a different format) and using     TFRecordWriter     to write the PredictionLog entries into     <code>YourSavedModel/assets.extra/tf_serving_warmup_requests</code>.</li> <li>By using TFX Infra Validator     option to export a Saved Model with warmup.     With this option the TFX Infa Validator will populate     <code>YourSavedModel/assets.extra/tf_serving_warmup_requests</code> based on the     validation requests provided via     RequestSpec.</li> </ul>"},{"location":"tutorials/tensorboard/","title":"Profile Inference Requests with TensorBoard","text":"<p>After deploying TensorFlow Serving and issuing requests from your client, you may notice that requests take longer than you expected, or you are not achieving the throughput that you would have liked.</p> <p>In this guide, we will use TensorBoard's Profiler, which you may already use to profile model training, to trace inference requests to help us debug and improve inference performance.</p> <p>You should use this guide in conjunction with the best practices denoted in the Performance Guide to optimize your model, requests and TensorFlow Serving instance.</p>"},{"location":"tutorials/tensorboard/#overview","title":"Overview","text":"<p>At a high level, we will point TensorBoard's Profiling tool at TensorFlow Serving's gRPC server. When we send an inference request to Tensorflow Serving, we will also simultaneously use the TensorBoard UI to ask it to capture the traces of this request. Behind the scenes, TensorBoard will talk to TensorFlow Serving over gRPC and ask it to provide a detailed trace of the lifetime of the inference request. TensorBoard will then visualize the activity of every thread on every compute device (running code integrated with <code>profiler::TraceMe</code>) over the course of the lifetime of the request on the TensorBoard UI for us to consume.</p>"},{"location":"tutorials/tensorboard/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>Tensorflow&gt;=2.0.0</code></li> <li>TensorBoard (should be installed if TF was installed via <code>pip</code>)</li> <li>Docker (which we'll use to download and run TF serving&gt;=2.1.0 image)</li> </ul>"},{"location":"tutorials/tensorboard/#deploy-model-with-tensorflow-serving","title":"Deploy model with TensorFlow Serving","text":"<p>For this example, we will use Docker, the recommended way to deploy Tensorflow Serving, to host a toy model that computes <code>f(x) = x / 2 + 2</code> found in the Tensorflow Serving Github repository.</p> <p>Download the TensorFlow Serving source.</p> <pre><code>git clone https://github.com/tensorflow/serving /tmp/serving\ncd /tmp/serving\n</code></pre> <p>Launch TensorFlow Serving via Docker and deploy the half_plus_two model.</p> <pre><code>docker pull tensorflow/serving\nMODELS_DIR=\"$(pwd)/tensorflow_serving/servables/tensorflow/testdata\"\ndocker run -it --rm -p 8500:8500 -p 8501:8501 \\\n-v $MODELS_DIR/saved_model_half_plus_two_cpu:/models/half_plus_two \\\n-v /tmp/tensorboard:/tmp/tensorboard \\\n-e MODEL_NAME=half_plus_two \\\ntensorflow/serving\n</code></pre> <p>In another terminal, query the model to ensure model is deployed correctly</p> <pre><code>curl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n-X POST http://localhost:8501/v1/models/half_plus_two:predict\n\n# Returns =&gt; { \"predictions\": [2.5, 3.0, 4.5] }\n</code></pre>"},{"location":"tutorials/tensorboard/#set-up-tensorboards-profiler","title":"Set up TensorBoard's Profiler","text":"<p>In another terminal, launch the TensorBoard tool on your machine, providing a directory to save the inference trace events to:</p> <pre><code>mkdir -p /tmp/tensorboard\ntensorboard --logdir /tmp/tensorboard --port 6006\n</code></pre> <p>Navigate to http://localhost:6006/ to view the TensorBoard UI. Use the drop-down menu at the top to navigate to the Profile tab. Click Capture Profile and provide the address of Tensorflow Serving's gRPC server.</p> <p></p> <p>As soon as you press \"Capture,\" TensorBoard will start sending profile requests to the model server. In the dialog above, you can set both the deadline for each request and the total number of times Tensorboard will retry if no trace events are collected. If you are profiling an expensive model, you may want to increase the deadline to ensure the profile request does not time out before the inference request completes.</p>"},{"location":"tutorials/tensorboard/#send-and-profile-an-inference-request","title":"Send and Profile an Inference Request","text":"<p>Press Capture on the TensorBoard UI and send an inference request to TF Serving quickly thereafter.</p> <pre><code>curl -d '{\"instances\": [1.0, 2.0, 5.0]}' -X POST \\\nhttp://localhost:8501/v1/models/half_plus_two:predict\n</code></pre> <p>You should see a \"Capture profile successfully. Please refresh.\" toast appear at the bottom of the screen. This means TensorBoard was able to retrieve trace events from TensorFlow Serving and saved them to your <code>logdir</code>. Refresh the page to visualize the inference request with The Profiler's Trace Viewer, as seen in the next section.</p> <p>Note: If you see <code>tensorflow.python.framework.errors_impl.UnimplementedError</code> in your TensorBoard logs, it likely means you are running a Tensorflow Serving version older than 2.1.</p>"},{"location":"tutorials/tensorboard/#analyze-the-inference-request-trace","title":"Analyze the Inference Request Trace","text":"<p>You can now easily see what computation is taking place as a result of your inference request. You can zoom and click on any of the rectangles (trace events) to get more information such as exact start time and wall duration.</p> <p>At a high-level, we see two threads belonging to the TensorFlow runtime and a third one that belongs to the REST server, handling the receiving of the HTTP request and creating a TensorFlow Session.</p> <p>We can zoom in to see what happens inside the SessionRun.</p> <p></p> <p>In the second thread, we see an initial ExecutorState::Process call in which no TensorFlow ops run but initialization steps are executed.</p> <p>In the first thread, we see the call to read the first variable, and once the second variable is also available, executes the multiplication and add kernels in sequence. Finally, the Executor signals that its computation is done by calling the DoneCallback and the Session can be closed.</p>"},{"location":"tutorials/tensorboard/#next-steps","title":"Next Steps","text":"<p>While this is a simple example, you can use the same process to profile much more complex models, allowing you to identify slow ops or bottlenecks in your model architecture to improve its performance.</p> <p>Please refer to TensorBoard Profiler Guide for a more complete tutorial on features of TensorBoard's Profiler and TensorFlow Serving Performance Guide to learn more about optimizing inference performance.</p>"}]}